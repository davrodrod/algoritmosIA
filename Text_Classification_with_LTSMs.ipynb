{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Classification with LTSMs.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOMm2INCfSNnabgKH8YWtJc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davrodrod/algoritmosIA/blob/master/Text_Classification_with_LTSMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DB6lqGQASoV9"
      },
      "source": [
        "Mismo problema de clasificación binario de movie reviews de IMDB pero ahora con LSTMs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eneHo9YlS4mI",
        "outputId": "1e0aa335-270e-4799-8dae-8716c609927d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Bidirectional, Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.datasets import imdb\n",
        "\n",
        "max_features = 20000\n",
        "maxlen = 100 # cut texts after this number of words (among top max_features most common words)\n",
        "batch_size = 32\n",
        "\n",
        "print(\"Loading data...\")\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "print(len(x_train), 'train sequences')\n",
        "print(len(x_test), 'test sequences')\n",
        "\n",
        "print('Pad sequences (samples x time)')\n",
        "x_train= sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test= sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "print('x_train shape', x_train.shape)\n",
        "print('x_test shape', x_test.shape)\n",
        "\n",
        "print('Build model...')\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 128))\n",
        "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# try using different optimizers and different optimize configs\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print(\"Train...\")\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=4,\n",
        "          validation_data=(x_test,y_test))\n",
        "score,acc = model.evaluate(x_test, y_test,\n",
        "                           batch_size=batch_size)\n",
        "print('Test score: ', score)\n",
        "print('Test accuracy: ', acc)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "25000 train sequences\n",
            "25000 test sequences\n",
            "Pad sequences (samples x time)\n",
            "x_train shape (25000, 100)\n",
            "x_test shape (25000, 100)\n",
            "Build model...\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Train...\n",
            "Epoch 1/4\n",
            "782/782 [==============================] - 327s 418ms/step - loss: 0.4165 - accuracy: 0.8089 - val_loss: 0.3740 - val_accuracy: 0.8381\n",
            "Epoch 2/4\n",
            "782/782 [==============================] - 331s 424ms/step - loss: 0.2444 - accuracy: 0.9037 - val_loss: 0.3616 - val_accuracy: 0.8504\n",
            "Epoch 3/4\n",
            "782/782 [==============================] - 331s 424ms/step - loss: 0.1553 - accuracy: 0.9430 - val_loss: 0.4700 - val_accuracy: 0.8373\n",
            "Epoch 4/4\n",
            "782/782 [==============================] - 332s 424ms/step - loss: 0.1137 - accuracy: 0.9594 - val_loss: 0.5191 - val_accuracy: 0.8380\n",
            "782/782 [==============================] - 22s 28ms/step - loss: 0.5191 - accuracy: 0.8380\n",
            "Test score:  0.5191226601600647\n",
            "Test accuracy:  0.8379999995231628\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYX2gMpkWbzR"
      },
      "source": [
        "El test accuracy es menor que con la red dense. Es porque tenemos un model muy potente y muy pocos datos de entrenamiento. No permite obtener toda la potencia.\n",
        "\n",
        "Puede verse como a partir de la primera época el loss está aumentando, llega a overfitting ya en la primera época. Tb se ve que el val acc no mejora.\n",
        "\n",
        "¿Por qué la loss empeora mucho y la accuracy se mantiene aproximadamente?\n",
        "\n",
        "Ejemplo claro de que no siempre el modelo más complejo funciona mejor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbcxIx1lXNIV"
      },
      "source": [
        "## Ahora con red bidireccional recurrente\n",
        "\n",
        "Aumenta mucho el coste computacional"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4prhM1NXTXQ",
        "outputId": "7a758801-9976-44cc-d64e-83daac74d97c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Bidirectional, Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.datasets import imdb\n",
        "\n",
        "max_features = 20000\n",
        "maxlen = 100 # cut texts after this number of words (among top max_features most common words)\n",
        "batch_size = 32\n",
        "\n",
        "print(\"Loading data...\")\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "print(len(x_train), 'train sequences')\n",
        "print(len(x_test), 'test sequences')\n",
        "\n",
        "print('Pad sequences (samples x time)')\n",
        "x_train= sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test= sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "print('x_train shape', x_train.shape)\n",
        "print('x_test shape', x_test.shape)\n",
        "\n",
        "print('Build model...')\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 128))\n",
        "model.add(Bidirectional(LSTM(16)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# try using different optimizers and different optimize configs\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print(\"Train...\")\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=4,\n",
        "          validation_data=(x_test,y_test))\n",
        "score,acc = model.evaluate(x_test, y_test,\n",
        "                           batch_size=batch_size)\n",
        "print('Test score: ', score)\n",
        "print('Test accuracy: ', acc)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "25000 train sequences\n",
            "25000 test sequences\n",
            "Pad sequences (samples x time)\n",
            "x_train shape (25000, 100)\n",
            "x_test shape (25000, 100)\n",
            "Build model...\n",
            "Train...\n",
            "Epoch 1/4\n",
            "782/782 [==============================] - 32s 41ms/step - loss: 0.4257 - accuracy: 0.8062 - val_loss: 0.3599 - val_accuracy: 0.8435\n",
            "Epoch 2/4\n",
            "782/782 [==============================] - 32s 40ms/step - loss: 0.2287 - accuracy: 0.9129 - val_loss: 0.3605 - val_accuracy: 0.8470\n",
            "Epoch 3/4\n",
            "782/782 [==============================] - 32s 41ms/step - loss: 0.1345 - accuracy: 0.9534 - val_loss: 0.4193 - val_accuracy: 0.8416\n",
            "Epoch 4/4\n",
            "782/782 [==============================] - 31s 40ms/step - loss: 0.0772 - accuracy: 0.9740 - val_loss: 0.5488 - val_accuracy: 0.8380\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.5488 - accuracy: 0.8380\n",
            "Test score:  0.5487738251686096\n",
            "Test accuracy:  0.8379600048065186\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5SIOC2MX44j"
      },
      "source": [
        "Modelo muy potente. Tb llega a overfitting ya en la primera epoch"
      ]
    }
  ]
}