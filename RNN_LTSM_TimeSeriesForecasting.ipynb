{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN_LTSM_TimeSeriesForecasting.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPHApK7aKxx/6mTKW2ne12J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davrodrod/algorirmosIA/blob/master/RNN_LTSM_TimeSeriesForecasting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyGwl0TJbDJT",
        "colab_type": "text"
      },
      "source": [
        "# Recurrent Neural Networks\n",
        "\n",
        "Este tipo de redes están diseñadas para operar sobre secuencias de datos\n",
        "\n",
        "Han probado ser muy efectivas para Procesado de Lenguaje Natural donde las secuncias de texto son las entradas. También han tenido algo de éxito en previsión de series de tiempo y en reconocimiento de habla.\n",
        "\n",
        "El tipo más popular de RNN es el LSTM (Long Short-Term Memory Network). Éstas pueden utilizarse para aceptar una secuencia de entradas y hacer una predicción, como una asignación a una clase o una predicción numérica como el siguiente valor en la secuencia.\n",
        "\n",
        "Se utiliza un dataset de venta de coches para predicciones univariable. El objetivo es predecir el número de ventas de coches por mes. \n",
        "\n",
        "Se utiliza una ventana de los últimos 5 meses para predecir los resultados del mes actual (función split_sequence())\n",
        "\n",
        "Por ejemplo, si la secuencia es: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10\n",
        "las muestras para entrenamiento serán:\n",
        "Input: 1, 2, 3, 4, 5   -> Output: 6\n",
        "Input: 2, 3, 4, 5, 6   -> Output: 7\n",
        "Input: 3, 4, 5, 6, 7   -> Output: 8\n",
        "...\n",
        "\n",
        "Dado que es un problema de regresión se utiliza una función de activación lineal (o lo que es lo mismo, no hay función de activación), y se optimiza el MSE. Tb se evalúa con el MAE\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOPF-RKrbE-l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "89996f64-1d81-4e2c-baec-4bd278d003ed"
      },
      "source": [
        "# Fuente https://machinelearningmastery.com/tensorflow-tutorial-deep-learning-with-tf-keras/\n",
        "\n",
        "# lstm for time series forecasting\n",
        "from numpy import sqrt\n",
        "from numpy import asarray\n",
        "from pandas import read_csv\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM\n",
        "\n",
        "# split a univariate sequence into samples\n",
        "def split_sequence(sequence, n_steps):\n",
        "        X, y = list(), list()\n",
        "        for i in range(len(sequence)):\n",
        "                # find the end of this pattern\n",
        "                end_ix = i + n_steps\n",
        "                # check if we are beyond the sequence\n",
        "                if end_ix > len(sequence)-1:\n",
        "                        break\n",
        "                # gather input and output parts of the pattern\n",
        "                seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
        "                X.append(seq_x)\n",
        "                y.append(seq_y)\n",
        "        return asarray(X), asarray(y)\n",
        "\n",
        "# load the dataset\n",
        "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/monthly-car-sales.csv'\n",
        "df = read_csv(path, header=0, index_col=0, squeeze=True)\n",
        "# retrieve the values\n",
        "values = df.values.astype('float32')\n",
        "# specify the window size\n",
        "n_steps = 5\n",
        "# split into samples\n",
        "X, y = split_sequence(values, n_steps)\n",
        "print(X)\n",
        "print(y)\n",
        "# reshape into [samples, timesteps, features]\n",
        "X = X.reshape((X.shape[0], X.shape[1], 1))\n",
        "print(X)\n",
        "# split into train/test\n",
        "n_test = 12\n",
        "X_train, X_test, y_train, y_test = X[:-n_test], X[-n_test:], y[:-n_test], y[-n_test:]\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
        "# define model\n",
        "model = Sequential()\n",
        "model.add(LSTM(100, activation='relu', kernel_initializer='he_normal', input_shape=(n_steps,1)))\n",
        "model.add(Dense(50, activation='relu', kernel_initializer='he_normal'))\n",
        "model.add(Dense(50, activation='relu', kernel_initializer='he_normal'))\n",
        "model.add(Dense(1))\n",
        "# compile the model\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "# fit the model\n",
        "model.fit(X_train, y_train, epochs=350, batch_size=32, verbose=2, validation_data=(X_test, y_test))\n",
        "# evaluate the model\n",
        "mse, mae = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('MSE: %.3f, RMSE: %.3f, MAE: %.3f' % (mse, sqrt(mse), mae))\n",
        "# make a prediction\n",
        "row = asarray([18024.0, 16722.0, 14385.0, 21342.0, 17180.0]).reshape((1, n_steps, 1))\n",
        "yhat = model.predict(row)\n",
        "print('Predicted: %.3f' % (yhat))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 6550.  8728. 12026. 14395. 14587.]\n",
            " [ 8728. 12026. 14395. 14587. 13791.]\n",
            " [12026. 14395. 14587. 13791.  9498.]\n",
            " [14395. 14587. 13791.  9498.  8251.]\n",
            " [14587. 13791.  9498.  8251.  7049.]\n",
            " [13791.  9498.  8251.  7049.  9545.]\n",
            " [ 9498.  8251.  7049.  9545.  9364.]\n",
            " [ 8251.  7049.  9545.  9364.  8456.]\n",
            " [ 7049.  9545.  9364.  8456.  7237.]\n",
            " [ 9545.  9364.  8456.  7237.  9374.]\n",
            " [ 9364.  8456.  7237.  9374. 11837.]\n",
            " [ 8456.  7237.  9374. 11837. 13784.]\n",
            " [ 7237.  9374. 11837. 13784. 15926.]\n",
            " [ 9374. 11837. 13784. 15926. 13821.]\n",
            " [11837. 13784. 15926. 13821. 11143.]\n",
            " [13784. 15926. 13821. 11143.  7975.]\n",
            " [15926. 13821. 11143.  7975.  7610.]\n",
            " [13821. 11143.  7975.  7610. 10015.]\n",
            " [11143.  7975.  7610. 10015. 12759.]\n",
            " [ 7975.  7610. 10015. 12759.  8816.]\n",
            " [ 7610. 10015. 12759.  8816. 10677.]\n",
            " [10015. 12759.  8816. 10677. 10947.]\n",
            " [12759.  8816. 10677. 10947. 15200.]\n",
            " [ 8816. 10677. 10947. 15200. 17010.]\n",
            " [10677. 10947. 15200. 17010. 20900.]\n",
            " [10947. 15200. 17010. 20900. 16205.]\n",
            " [15200. 17010. 20900. 16205. 12143.]\n",
            " [17010. 20900. 16205. 12143.  8997.]\n",
            " [20900. 16205. 12143.  8997.  5568.]\n",
            " [16205. 12143.  8997.  5568. 11474.]\n",
            " [12143.  8997.  5568. 11474. 12256.]\n",
            " [ 8997.  5568. 11474. 12256. 10583.]\n",
            " [ 5568. 11474. 12256. 10583. 10862.]\n",
            " [11474. 12256. 10583. 10862. 10965.]\n",
            " [12256. 10583. 10862. 10965. 14405.]\n",
            " [10583. 10862. 10965. 14405. 20379.]\n",
            " [10862. 10965. 14405. 20379. 20128.]\n",
            " [10965. 14405. 20379. 20128. 17816.]\n",
            " [14405. 20379. 20128. 17816. 12268.]\n",
            " [20379. 20128. 17816. 12268.  8642.]\n",
            " [20128. 17816. 12268.  8642.  7962.]\n",
            " [17816. 12268.  8642.  7962. 13932.]\n",
            " [12268.  8642.  7962. 13932. 15936.]\n",
            " [ 8642.  7962. 13932. 15936. 12628.]\n",
            " [ 7962. 13932. 15936. 12628. 12267.]\n",
            " [13932. 15936. 12628. 12267. 12470.]\n",
            " [15936. 12628. 12267. 12470. 18944.]\n",
            " [12628. 12267. 12470. 18944. 21259.]\n",
            " [12267. 12470. 18944. 21259. 22015.]\n",
            " [12470. 18944. 21259. 22015. 18581.]\n",
            " [18944. 21259. 22015. 18581. 15175.]\n",
            " [21259. 22015. 18581. 15175. 10306.]\n",
            " [22015. 18581. 15175. 10306. 10792.]\n",
            " [18581. 15175. 10306. 10792. 14752.]\n",
            " [15175. 10306. 10792. 14752. 13754.]\n",
            " [10306. 10792. 14752. 13754. 11738.]\n",
            " [10792. 14752. 13754. 11738. 12181.]\n",
            " [14752. 13754. 11738. 12181. 12965.]\n",
            " [13754. 11738. 12181. 12965. 19990.]\n",
            " [11738. 12181. 12965. 19990. 23125.]\n",
            " [12181. 12965. 19990. 23125. 23541.]\n",
            " [12965. 19990. 23125. 23541. 21247.]\n",
            " [19990. 23125. 23541. 21247. 15189.]\n",
            " [23125. 23541. 21247. 15189. 14767.]\n",
            " [23541. 21247. 15189. 14767. 10895.]\n",
            " [21247. 15189. 14767. 10895. 17130.]\n",
            " [15189. 14767. 10895. 17130. 17697.]\n",
            " [14767. 10895. 17130. 17697. 16611.]\n",
            " [10895. 17130. 17697. 16611. 12674.]\n",
            " [17130. 17697. 16611. 12674. 12760.]\n",
            " [17697. 16611. 12674. 12760. 20249.]\n",
            " [16611. 12674. 12760. 20249. 22135.]\n",
            " [12674. 12760. 20249. 22135. 20677.]\n",
            " [12760. 20249. 22135. 20677. 19933.]\n",
            " [20249. 22135. 20677. 19933. 15388.]\n",
            " [22135. 20677. 19933. 15388. 15113.]\n",
            " [20677. 19933. 15388. 15113. 13401.]\n",
            " [19933. 15388. 15113. 13401. 16135.]\n",
            " [15388. 15113. 13401. 16135. 17562.]\n",
            " [15113. 13401. 16135. 17562. 14720.]\n",
            " [13401. 16135. 17562. 14720. 12225.]\n",
            " [16135. 17562. 14720. 12225. 11608.]\n",
            " [17562. 14720. 12225. 11608. 20985.]\n",
            " [14720. 12225. 11608. 20985. 19692.]\n",
            " [12225. 11608. 20985. 19692. 24081.]\n",
            " [11608. 20985. 19692. 24081. 22114.]\n",
            " [20985. 19692. 24081. 22114. 14220.]\n",
            " [19692. 24081. 22114. 14220. 13434.]\n",
            " [24081. 22114. 14220. 13434. 13598.]\n",
            " [22114. 14220. 13434. 13598. 17187.]\n",
            " [14220. 13434. 13598. 17187. 16119.]\n",
            " [13434. 13598. 17187. 16119. 13713.]\n",
            " [13598. 17187. 16119. 13713. 13210.]\n",
            " [17187. 16119. 13713. 13210. 14251.]\n",
            " [16119. 13713. 13210. 14251. 20139.]\n",
            " [13713. 13210. 14251. 20139. 21725.]\n",
            " [13210. 14251. 20139. 21725. 26099.]\n",
            " [14251. 20139. 21725. 26099. 21084.]\n",
            " [20139. 21725. 26099. 21084. 18024.]\n",
            " [21725. 26099. 21084. 18024. 16722.]\n",
            " [26099. 21084. 18024. 16722. 14385.]\n",
            " [21084. 18024. 16722. 14385. 21342.]\n",
            " [18024. 16722. 14385. 21342. 17180.]]\n",
            "[13791.  9498.  8251.  7049.  9545.  9364.  8456.  7237.  9374. 11837.\n",
            " 13784. 15926. 13821. 11143.  7975.  7610. 10015. 12759.  8816. 10677.\n",
            " 10947. 15200. 17010. 20900. 16205. 12143.  8997.  5568. 11474. 12256.\n",
            " 10583. 10862. 10965. 14405. 20379. 20128. 17816. 12268.  8642.  7962.\n",
            " 13932. 15936. 12628. 12267. 12470. 18944. 21259. 22015. 18581. 15175.\n",
            " 10306. 10792. 14752. 13754. 11738. 12181. 12965. 19990. 23125. 23541.\n",
            " 21247. 15189. 14767. 10895. 17130. 17697. 16611. 12674. 12760. 20249.\n",
            " 22135. 20677. 19933. 15388. 15113. 13401. 16135. 17562. 14720. 12225.\n",
            " 11608. 20985. 19692. 24081. 22114. 14220. 13434. 13598. 17187. 16119.\n",
            " 13713. 13210. 14251. 20139. 21725. 26099. 21084. 18024. 16722. 14385.\n",
            " 21342. 17180. 14577.]\n",
            "[[[ 6550.]\n",
            "  [ 8728.]\n",
            "  [12026.]\n",
            "  [14395.]\n",
            "  [14587.]]\n",
            "\n",
            " [[ 8728.]\n",
            "  [12026.]\n",
            "  [14395.]\n",
            "  [14587.]\n",
            "  [13791.]]\n",
            "\n",
            " [[12026.]\n",
            "  [14395.]\n",
            "  [14587.]\n",
            "  [13791.]\n",
            "  [ 9498.]]\n",
            "\n",
            " [[14395.]\n",
            "  [14587.]\n",
            "  [13791.]\n",
            "  [ 9498.]\n",
            "  [ 8251.]]\n",
            "\n",
            " [[14587.]\n",
            "  [13791.]\n",
            "  [ 9498.]\n",
            "  [ 8251.]\n",
            "  [ 7049.]]\n",
            "\n",
            " [[13791.]\n",
            "  [ 9498.]\n",
            "  [ 8251.]\n",
            "  [ 7049.]\n",
            "  [ 9545.]]\n",
            "\n",
            " [[ 9498.]\n",
            "  [ 8251.]\n",
            "  [ 7049.]\n",
            "  [ 9545.]\n",
            "  [ 9364.]]\n",
            "\n",
            " [[ 8251.]\n",
            "  [ 7049.]\n",
            "  [ 9545.]\n",
            "  [ 9364.]\n",
            "  [ 8456.]]\n",
            "\n",
            " [[ 7049.]\n",
            "  [ 9545.]\n",
            "  [ 9364.]\n",
            "  [ 8456.]\n",
            "  [ 7237.]]\n",
            "\n",
            " [[ 9545.]\n",
            "  [ 9364.]\n",
            "  [ 8456.]\n",
            "  [ 7237.]\n",
            "  [ 9374.]]\n",
            "\n",
            " [[ 9364.]\n",
            "  [ 8456.]\n",
            "  [ 7237.]\n",
            "  [ 9374.]\n",
            "  [11837.]]\n",
            "\n",
            " [[ 8456.]\n",
            "  [ 7237.]\n",
            "  [ 9374.]\n",
            "  [11837.]\n",
            "  [13784.]]\n",
            "\n",
            " [[ 7237.]\n",
            "  [ 9374.]\n",
            "  [11837.]\n",
            "  [13784.]\n",
            "  [15926.]]\n",
            "\n",
            " [[ 9374.]\n",
            "  [11837.]\n",
            "  [13784.]\n",
            "  [15926.]\n",
            "  [13821.]]\n",
            "\n",
            " [[11837.]\n",
            "  [13784.]\n",
            "  [15926.]\n",
            "  [13821.]\n",
            "  [11143.]]\n",
            "\n",
            " [[13784.]\n",
            "  [15926.]\n",
            "  [13821.]\n",
            "  [11143.]\n",
            "  [ 7975.]]\n",
            "\n",
            " [[15926.]\n",
            "  [13821.]\n",
            "  [11143.]\n",
            "  [ 7975.]\n",
            "  [ 7610.]]\n",
            "\n",
            " [[13821.]\n",
            "  [11143.]\n",
            "  [ 7975.]\n",
            "  [ 7610.]\n",
            "  [10015.]]\n",
            "\n",
            " [[11143.]\n",
            "  [ 7975.]\n",
            "  [ 7610.]\n",
            "  [10015.]\n",
            "  [12759.]]\n",
            "\n",
            " [[ 7975.]\n",
            "  [ 7610.]\n",
            "  [10015.]\n",
            "  [12759.]\n",
            "  [ 8816.]]\n",
            "\n",
            " [[ 7610.]\n",
            "  [10015.]\n",
            "  [12759.]\n",
            "  [ 8816.]\n",
            "  [10677.]]\n",
            "\n",
            " [[10015.]\n",
            "  [12759.]\n",
            "  [ 8816.]\n",
            "  [10677.]\n",
            "  [10947.]]\n",
            "\n",
            " [[12759.]\n",
            "  [ 8816.]\n",
            "  [10677.]\n",
            "  [10947.]\n",
            "  [15200.]]\n",
            "\n",
            " [[ 8816.]\n",
            "  [10677.]\n",
            "  [10947.]\n",
            "  [15200.]\n",
            "  [17010.]]\n",
            "\n",
            " [[10677.]\n",
            "  [10947.]\n",
            "  [15200.]\n",
            "  [17010.]\n",
            "  [20900.]]\n",
            "\n",
            " [[10947.]\n",
            "  [15200.]\n",
            "  [17010.]\n",
            "  [20900.]\n",
            "  [16205.]]\n",
            "\n",
            " [[15200.]\n",
            "  [17010.]\n",
            "  [20900.]\n",
            "  [16205.]\n",
            "  [12143.]]\n",
            "\n",
            " [[17010.]\n",
            "  [20900.]\n",
            "  [16205.]\n",
            "  [12143.]\n",
            "  [ 8997.]]\n",
            "\n",
            " [[20900.]\n",
            "  [16205.]\n",
            "  [12143.]\n",
            "  [ 8997.]\n",
            "  [ 5568.]]\n",
            "\n",
            " [[16205.]\n",
            "  [12143.]\n",
            "  [ 8997.]\n",
            "  [ 5568.]\n",
            "  [11474.]]\n",
            "\n",
            " [[12143.]\n",
            "  [ 8997.]\n",
            "  [ 5568.]\n",
            "  [11474.]\n",
            "  [12256.]]\n",
            "\n",
            " [[ 8997.]\n",
            "  [ 5568.]\n",
            "  [11474.]\n",
            "  [12256.]\n",
            "  [10583.]]\n",
            "\n",
            " [[ 5568.]\n",
            "  [11474.]\n",
            "  [12256.]\n",
            "  [10583.]\n",
            "  [10862.]]\n",
            "\n",
            " [[11474.]\n",
            "  [12256.]\n",
            "  [10583.]\n",
            "  [10862.]\n",
            "  [10965.]]\n",
            "\n",
            " [[12256.]\n",
            "  [10583.]\n",
            "  [10862.]\n",
            "  [10965.]\n",
            "  [14405.]]\n",
            "\n",
            " [[10583.]\n",
            "  [10862.]\n",
            "  [10965.]\n",
            "  [14405.]\n",
            "  [20379.]]\n",
            "\n",
            " [[10862.]\n",
            "  [10965.]\n",
            "  [14405.]\n",
            "  [20379.]\n",
            "  [20128.]]\n",
            "\n",
            " [[10965.]\n",
            "  [14405.]\n",
            "  [20379.]\n",
            "  [20128.]\n",
            "  [17816.]]\n",
            "\n",
            " [[14405.]\n",
            "  [20379.]\n",
            "  [20128.]\n",
            "  [17816.]\n",
            "  [12268.]]\n",
            "\n",
            " [[20379.]\n",
            "  [20128.]\n",
            "  [17816.]\n",
            "  [12268.]\n",
            "  [ 8642.]]\n",
            "\n",
            " [[20128.]\n",
            "  [17816.]\n",
            "  [12268.]\n",
            "  [ 8642.]\n",
            "  [ 7962.]]\n",
            "\n",
            " [[17816.]\n",
            "  [12268.]\n",
            "  [ 8642.]\n",
            "  [ 7962.]\n",
            "  [13932.]]\n",
            "\n",
            " [[12268.]\n",
            "  [ 8642.]\n",
            "  [ 7962.]\n",
            "  [13932.]\n",
            "  [15936.]]\n",
            "\n",
            " [[ 8642.]\n",
            "  [ 7962.]\n",
            "  [13932.]\n",
            "  [15936.]\n",
            "  [12628.]]\n",
            "\n",
            " [[ 7962.]\n",
            "  [13932.]\n",
            "  [15936.]\n",
            "  [12628.]\n",
            "  [12267.]]\n",
            "\n",
            " [[13932.]\n",
            "  [15936.]\n",
            "  [12628.]\n",
            "  [12267.]\n",
            "  [12470.]]\n",
            "\n",
            " [[15936.]\n",
            "  [12628.]\n",
            "  [12267.]\n",
            "  [12470.]\n",
            "  [18944.]]\n",
            "\n",
            " [[12628.]\n",
            "  [12267.]\n",
            "  [12470.]\n",
            "  [18944.]\n",
            "  [21259.]]\n",
            "\n",
            " [[12267.]\n",
            "  [12470.]\n",
            "  [18944.]\n",
            "  [21259.]\n",
            "  [22015.]]\n",
            "\n",
            " [[12470.]\n",
            "  [18944.]\n",
            "  [21259.]\n",
            "  [22015.]\n",
            "  [18581.]]\n",
            "\n",
            " [[18944.]\n",
            "  [21259.]\n",
            "  [22015.]\n",
            "  [18581.]\n",
            "  [15175.]]\n",
            "\n",
            " [[21259.]\n",
            "  [22015.]\n",
            "  [18581.]\n",
            "  [15175.]\n",
            "  [10306.]]\n",
            "\n",
            " [[22015.]\n",
            "  [18581.]\n",
            "  [15175.]\n",
            "  [10306.]\n",
            "  [10792.]]\n",
            "\n",
            " [[18581.]\n",
            "  [15175.]\n",
            "  [10306.]\n",
            "  [10792.]\n",
            "  [14752.]]\n",
            "\n",
            " [[15175.]\n",
            "  [10306.]\n",
            "  [10792.]\n",
            "  [14752.]\n",
            "  [13754.]]\n",
            "\n",
            " [[10306.]\n",
            "  [10792.]\n",
            "  [14752.]\n",
            "  [13754.]\n",
            "  [11738.]]\n",
            "\n",
            " [[10792.]\n",
            "  [14752.]\n",
            "  [13754.]\n",
            "  [11738.]\n",
            "  [12181.]]\n",
            "\n",
            " [[14752.]\n",
            "  [13754.]\n",
            "  [11738.]\n",
            "  [12181.]\n",
            "  [12965.]]\n",
            "\n",
            " [[13754.]\n",
            "  [11738.]\n",
            "  [12181.]\n",
            "  [12965.]\n",
            "  [19990.]]\n",
            "\n",
            " [[11738.]\n",
            "  [12181.]\n",
            "  [12965.]\n",
            "  [19990.]\n",
            "  [23125.]]\n",
            "\n",
            " [[12181.]\n",
            "  [12965.]\n",
            "  [19990.]\n",
            "  [23125.]\n",
            "  [23541.]]\n",
            "\n",
            " [[12965.]\n",
            "  [19990.]\n",
            "  [23125.]\n",
            "  [23541.]\n",
            "  [21247.]]\n",
            "\n",
            " [[19990.]\n",
            "  [23125.]\n",
            "  [23541.]\n",
            "  [21247.]\n",
            "  [15189.]]\n",
            "\n",
            " [[23125.]\n",
            "  [23541.]\n",
            "  [21247.]\n",
            "  [15189.]\n",
            "  [14767.]]\n",
            "\n",
            " [[23541.]\n",
            "  [21247.]\n",
            "  [15189.]\n",
            "  [14767.]\n",
            "  [10895.]]\n",
            "\n",
            " [[21247.]\n",
            "  [15189.]\n",
            "  [14767.]\n",
            "  [10895.]\n",
            "  [17130.]]\n",
            "\n",
            " [[15189.]\n",
            "  [14767.]\n",
            "  [10895.]\n",
            "  [17130.]\n",
            "  [17697.]]\n",
            "\n",
            " [[14767.]\n",
            "  [10895.]\n",
            "  [17130.]\n",
            "  [17697.]\n",
            "  [16611.]]\n",
            "\n",
            " [[10895.]\n",
            "  [17130.]\n",
            "  [17697.]\n",
            "  [16611.]\n",
            "  [12674.]]\n",
            "\n",
            " [[17130.]\n",
            "  [17697.]\n",
            "  [16611.]\n",
            "  [12674.]\n",
            "  [12760.]]\n",
            "\n",
            " [[17697.]\n",
            "  [16611.]\n",
            "  [12674.]\n",
            "  [12760.]\n",
            "  [20249.]]\n",
            "\n",
            " [[16611.]\n",
            "  [12674.]\n",
            "  [12760.]\n",
            "  [20249.]\n",
            "  [22135.]]\n",
            "\n",
            " [[12674.]\n",
            "  [12760.]\n",
            "  [20249.]\n",
            "  [22135.]\n",
            "  [20677.]]\n",
            "\n",
            " [[12760.]\n",
            "  [20249.]\n",
            "  [22135.]\n",
            "  [20677.]\n",
            "  [19933.]]\n",
            "\n",
            " [[20249.]\n",
            "  [22135.]\n",
            "  [20677.]\n",
            "  [19933.]\n",
            "  [15388.]]\n",
            "\n",
            " [[22135.]\n",
            "  [20677.]\n",
            "  [19933.]\n",
            "  [15388.]\n",
            "  [15113.]]\n",
            "\n",
            " [[20677.]\n",
            "  [19933.]\n",
            "  [15388.]\n",
            "  [15113.]\n",
            "  [13401.]]\n",
            "\n",
            " [[19933.]\n",
            "  [15388.]\n",
            "  [15113.]\n",
            "  [13401.]\n",
            "  [16135.]]\n",
            "\n",
            " [[15388.]\n",
            "  [15113.]\n",
            "  [13401.]\n",
            "  [16135.]\n",
            "  [17562.]]\n",
            "\n",
            " [[15113.]\n",
            "  [13401.]\n",
            "  [16135.]\n",
            "  [17562.]\n",
            "  [14720.]]\n",
            "\n",
            " [[13401.]\n",
            "  [16135.]\n",
            "  [17562.]\n",
            "  [14720.]\n",
            "  [12225.]]\n",
            "\n",
            " [[16135.]\n",
            "  [17562.]\n",
            "  [14720.]\n",
            "  [12225.]\n",
            "  [11608.]]\n",
            "\n",
            " [[17562.]\n",
            "  [14720.]\n",
            "  [12225.]\n",
            "  [11608.]\n",
            "  [20985.]]\n",
            "\n",
            " [[14720.]\n",
            "  [12225.]\n",
            "  [11608.]\n",
            "  [20985.]\n",
            "  [19692.]]\n",
            "\n",
            " [[12225.]\n",
            "  [11608.]\n",
            "  [20985.]\n",
            "  [19692.]\n",
            "  [24081.]]\n",
            "\n",
            " [[11608.]\n",
            "  [20985.]\n",
            "  [19692.]\n",
            "  [24081.]\n",
            "  [22114.]]\n",
            "\n",
            " [[20985.]\n",
            "  [19692.]\n",
            "  [24081.]\n",
            "  [22114.]\n",
            "  [14220.]]\n",
            "\n",
            " [[19692.]\n",
            "  [24081.]\n",
            "  [22114.]\n",
            "  [14220.]\n",
            "  [13434.]]\n",
            "\n",
            " [[24081.]\n",
            "  [22114.]\n",
            "  [14220.]\n",
            "  [13434.]\n",
            "  [13598.]]\n",
            "\n",
            " [[22114.]\n",
            "  [14220.]\n",
            "  [13434.]\n",
            "  [13598.]\n",
            "  [17187.]]\n",
            "\n",
            " [[14220.]\n",
            "  [13434.]\n",
            "  [13598.]\n",
            "  [17187.]\n",
            "  [16119.]]\n",
            "\n",
            " [[13434.]\n",
            "  [13598.]\n",
            "  [17187.]\n",
            "  [16119.]\n",
            "  [13713.]]\n",
            "\n",
            " [[13598.]\n",
            "  [17187.]\n",
            "  [16119.]\n",
            "  [13713.]\n",
            "  [13210.]]\n",
            "\n",
            " [[17187.]\n",
            "  [16119.]\n",
            "  [13713.]\n",
            "  [13210.]\n",
            "  [14251.]]\n",
            "\n",
            " [[16119.]\n",
            "  [13713.]\n",
            "  [13210.]\n",
            "  [14251.]\n",
            "  [20139.]]\n",
            "\n",
            " [[13713.]\n",
            "  [13210.]\n",
            "  [14251.]\n",
            "  [20139.]\n",
            "  [21725.]]\n",
            "\n",
            " [[13210.]\n",
            "  [14251.]\n",
            "  [20139.]\n",
            "  [21725.]\n",
            "  [26099.]]\n",
            "\n",
            " [[14251.]\n",
            "  [20139.]\n",
            "  [21725.]\n",
            "  [26099.]\n",
            "  [21084.]]\n",
            "\n",
            " [[20139.]\n",
            "  [21725.]\n",
            "  [26099.]\n",
            "  [21084.]\n",
            "  [18024.]]\n",
            "\n",
            " [[21725.]\n",
            "  [26099.]\n",
            "  [21084.]\n",
            "  [18024.]\n",
            "  [16722.]]\n",
            "\n",
            " [[26099.]\n",
            "  [21084.]\n",
            "  [18024.]\n",
            "  [16722.]\n",
            "  [14385.]]\n",
            "\n",
            " [[21084.]\n",
            "  [18024.]\n",
            "  [16722.]\n",
            "  [14385.]\n",
            "  [21342.]]\n",
            "\n",
            " [[18024.]\n",
            "  [16722.]\n",
            "  [14385.]\n",
            "  [21342.]\n",
            "  [17180.]]]\n",
            "(91, 5, 1) (12, 5, 1) (91,) (12,)\n",
            "Epoch 1/350\n",
            "3/3 - 0s - loss: 32780882.0000 - mae: 4558.6602 - val_loss: 20566450.0000 - val_mae: 3418.4629\n",
            "Epoch 2/350\n",
            "3/3 - 0s - loss: 16248749.0000 - mae: 3265.4368 - val_loss: 18340558.0000 - val_mae: 3657.9500\n",
            "Epoch 3/350\n",
            "3/3 - 0s - loss: 16914938.0000 - mae: 3442.2012 - val_loss: 14106831.0000 - val_mae: 3080.7195\n",
            "Epoch 4/350\n",
            "3/3 - 0s - loss: 12774275.0000 - mae: 2821.6865 - val_loss: 16428541.0000 - val_mae: 3245.8079\n",
            "Epoch 5/350\n",
            "3/3 - 0s - loss: 12662412.0000 - mae: 2853.7112 - val_loss: 16439171.0000 - val_mae: 3403.0879\n",
            "Epoch 6/350\n",
            "3/3 - 0s - loss: 12122300.0000 - mae: 2772.1257 - val_loss: 12140032.0000 - val_mae: 3026.3708\n",
            "Epoch 7/350\n",
            "3/3 - 0s - loss: 9785574.0000 - mae: 2515.3445 - val_loss: 11131455.0000 - val_mae: 2807.1113\n",
            "Epoch 8/350\n",
            "3/3 - 0s - loss: 10291439.0000 - mae: 2525.7048 - val_loss: 10683278.0000 - val_mae: 2875.1992\n",
            "Epoch 9/350\n",
            "3/3 - 0s - loss: 9045768.0000 - mae: 2416.0457 - val_loss: 9644314.0000 - val_mae: 2681.6926\n",
            "Epoch 10/350\n",
            "3/3 - 0s - loss: 8514237.0000 - mae: 2444.1016 - val_loss: 8668352.0000 - val_mae: 2480.1938\n",
            "Epoch 11/350\n",
            "3/3 - 0s - loss: 8365922.5000 - mae: 2301.2017 - val_loss: 9227097.0000 - val_mae: 2408.6072\n",
            "Epoch 12/350\n",
            "3/3 - 0s - loss: 7990117.5000 - mae: 2245.2444 - val_loss: 7878315.5000 - val_mae: 2420.0081\n",
            "Epoch 13/350\n",
            "3/3 - 0s - loss: 8101681.5000 - mae: 2331.3811 - val_loss: 7925864.0000 - val_mae: 2377.4167\n",
            "Epoch 14/350\n",
            "3/3 - 0s - loss: 8960105.0000 - mae: 2278.5559 - val_loss: 12144880.0000 - val_mae: 2690.1289\n",
            "Epoch 15/350\n",
            "3/3 - 0s - loss: 9426540.0000 - mae: 2315.3193 - val_loss: 11070627.0000 - val_mae: 2372.2297\n",
            "Epoch 16/350\n",
            "3/3 - 0s - loss: 9535183.0000 - mae: 2323.4788 - val_loss: 11515041.0000 - val_mae: 2686.7422\n",
            "Epoch 17/350\n",
            "3/3 - 0s - loss: 9116392.0000 - mae: 2329.9961 - val_loss: 13434915.0000 - val_mae: 2713.6003\n",
            "Epoch 18/350\n",
            "3/3 - 0s - loss: 10233267.0000 - mae: 2524.0554 - val_loss: 10404213.0000 - val_mae: 2693.2375\n",
            "Epoch 19/350\n",
            "3/3 - 0s - loss: 9560340.0000 - mae: 2508.4431 - val_loss: 9297857.0000 - val_mae: 2586.1985\n",
            "Epoch 20/350\n",
            "3/3 - 0s - loss: 9436928.0000 - mae: 2419.8003 - val_loss: 11826792.0000 - val_mae: 2730.8877\n",
            "Epoch 21/350\n",
            "3/3 - 0s - loss: 8945041.0000 - mae: 2365.4966 - val_loss: 9036056.0000 - val_mae: 2535.0730\n",
            "Epoch 22/350\n",
            "3/3 - 0s - loss: 8662740.0000 - mae: 2399.8723 - val_loss: 8918299.0000 - val_mae: 2505.2239\n",
            "Epoch 23/350\n",
            "3/3 - 0s - loss: 8969855.0000 - mae: 2407.8604 - val_loss: 10119723.0000 - val_mae: 2620.9021\n",
            "Epoch 24/350\n",
            "3/3 - 0s - loss: 9854820.0000 - mae: 2492.9832 - val_loss: 11217149.0000 - val_mae: 2942.9241\n",
            "Epoch 25/350\n",
            "3/3 - 0s - loss: 9021884.0000 - mae: 2495.5500 - val_loss: 10725715.0000 - val_mae: 2723.1338\n",
            "Epoch 26/350\n",
            "3/3 - 0s - loss: 9416757.0000 - mae: 2457.1316 - val_loss: 9082025.0000 - val_mae: 2438.4612\n",
            "Epoch 27/350\n",
            "3/3 - 0s - loss: 9539789.0000 - mae: 2330.2222 - val_loss: 7850622.0000 - val_mae: 2332.2732\n",
            "Epoch 28/350\n",
            "3/3 - 0s - loss: 9523047.0000 - mae: 2388.9036 - val_loss: 9988548.0000 - val_mae: 2605.8259\n",
            "Epoch 29/350\n",
            "3/3 - 0s - loss: 9210041.0000 - mae: 2354.1252 - val_loss: 9571788.0000 - val_mae: 2417.4329\n",
            "Epoch 30/350\n",
            "3/3 - 0s - loss: 8697759.0000 - mae: 2355.4448 - val_loss: 10395951.0000 - val_mae: 2583.2725\n",
            "Epoch 31/350\n",
            "3/3 - 0s - loss: 8634510.0000 - mae: 2298.3938 - val_loss: 10743343.0000 - val_mae: 2692.0762\n",
            "Epoch 32/350\n",
            "3/3 - 0s - loss: 8725368.0000 - mae: 2296.1470 - val_loss: 10743047.0000 - val_mae: 2706.1541\n",
            "Epoch 33/350\n",
            "3/3 - 0s - loss: 8967921.0000 - mae: 2382.9343 - val_loss: 11784748.0000 - val_mae: 2761.8967\n",
            "Epoch 34/350\n",
            "3/3 - 0s - loss: 8373986.5000 - mae: 2257.1658 - val_loss: 13965625.0000 - val_mae: 3174.3635\n",
            "Epoch 35/350\n",
            "3/3 - 0s - loss: 8353084.0000 - mae: 2283.8660 - val_loss: 12037157.0000 - val_mae: 2933.7776\n",
            "Epoch 36/350\n",
            "3/3 - 0s - loss: 8686671.0000 - mae: 2294.0715 - val_loss: 14499611.0000 - val_mae: 3093.4011\n",
            "Epoch 37/350\n",
            "3/3 - 0s - loss: 9983703.0000 - mae: 2512.9639 - val_loss: 12579501.0000 - val_mae: 2782.1575\n",
            "Epoch 38/350\n",
            "3/3 - 0s - loss: 9379929.0000 - mae: 2378.9041 - val_loss: 10801632.0000 - val_mae: 2554.5942\n",
            "Epoch 39/350\n",
            "3/3 - 0s - loss: 10013919.0000 - mae: 2472.2139 - val_loss: 10424285.0000 - val_mae: 2705.5959\n",
            "Epoch 40/350\n",
            "3/3 - 0s - loss: 9417227.0000 - mae: 2443.5212 - val_loss: 12691872.0000 - val_mae: 2865.4326\n",
            "Epoch 41/350\n",
            "3/3 - 0s - loss: 8520619.0000 - mae: 2324.1924 - val_loss: 11635843.0000 - val_mae: 2699.1963\n",
            "Epoch 42/350\n",
            "3/3 - 0s - loss: 7668782.0000 - mae: 2174.2566 - val_loss: 11689283.0000 - val_mae: 2827.4170\n",
            "Epoch 43/350\n",
            "3/3 - 0s - loss: 7627242.0000 - mae: 2164.1855 - val_loss: 12533903.0000 - val_mae: 2909.8586\n",
            "Epoch 44/350\n",
            "3/3 - 0s - loss: 7855638.5000 - mae: 2133.7363 - val_loss: 11941712.0000 - val_mae: 2754.4387\n",
            "Epoch 45/350\n",
            "3/3 - 0s - loss: 8186724.0000 - mae: 2216.4507 - val_loss: 11391832.0000 - val_mae: 2647.8252\n",
            "Epoch 46/350\n",
            "3/3 - 0s - loss: 7537444.0000 - mae: 2118.7026 - val_loss: 11752013.0000 - val_mae: 2658.7141\n",
            "Epoch 47/350\n",
            "3/3 - 0s - loss: 7213870.5000 - mae: 2060.9131 - val_loss: 11345384.0000 - val_mae: 2632.7397\n",
            "Epoch 48/350\n",
            "3/3 - 0s - loss: 7162535.5000 - mae: 2047.8132 - val_loss: 11043501.0000 - val_mae: 2590.7693\n",
            "Epoch 49/350\n",
            "3/3 - 0s - loss: 7338432.0000 - mae: 2069.9165 - val_loss: 11449849.0000 - val_mae: 2605.1711\n",
            "Epoch 50/350\n",
            "3/3 - 0s - loss: 7785253.5000 - mae: 2174.0791 - val_loss: 12010191.0000 - val_mae: 2638.5781\n",
            "Epoch 51/350\n",
            "3/3 - 0s - loss: 7742080.5000 - mae: 2077.7949 - val_loss: 11541975.0000 - val_mae: 2618.1057\n",
            "Epoch 52/350\n",
            "3/3 - 0s - loss: 7218381.5000 - mae: 2082.8271 - val_loss: 12178005.0000 - val_mae: 2814.0730\n",
            "Epoch 53/350\n",
            "3/3 - 0s - loss: 7754343.5000 - mae: 2234.0774 - val_loss: 12283517.0000 - val_mae: 2851.1660\n",
            "Epoch 54/350\n",
            "3/3 - 0s - loss: 7830418.0000 - mae: 2114.3828 - val_loss: 11867752.0000 - val_mae: 2931.9592\n",
            "Epoch 55/350\n",
            "3/3 - 0s - loss: 7486029.5000 - mae: 2142.6775 - val_loss: 12808728.0000 - val_mae: 2989.9873\n",
            "Epoch 56/350\n",
            "3/3 - 0s - loss: 8026032.0000 - mae: 2291.3486 - val_loss: 14001112.0000 - val_mae: 3179.4846\n",
            "Epoch 57/350\n",
            "3/3 - 0s - loss: 10919245.0000 - mae: 2558.7908 - val_loss: 13464955.0000 - val_mae: 3132.2151\n",
            "Epoch 58/350\n",
            "3/3 - 0s - loss: 8439456.0000 - mae: 2295.8125 - val_loss: 13606167.0000 - val_mae: 3127.8652\n",
            "Epoch 59/350\n",
            "3/3 - 0s - loss: 9621677.0000 - mae: 2443.2188 - val_loss: 16466507.0000 - val_mae: 3239.4524\n",
            "Epoch 60/350\n",
            "3/3 - 0s - loss: 8225133.5000 - mae: 2135.6125 - val_loss: 11484888.0000 - val_mae: 2918.0710\n",
            "Epoch 61/350\n",
            "3/3 - 0s - loss: 8542229.0000 - mae: 2424.2778 - val_loss: 11066505.0000 - val_mae: 2881.3057\n",
            "Epoch 62/350\n",
            "3/3 - 0s - loss: 7394937.5000 - mae: 2070.4741 - val_loss: 12898368.0000 - val_mae: 2903.4785\n",
            "Epoch 63/350\n",
            "3/3 - 0s - loss: 7487993.5000 - mae: 2089.9731 - val_loss: 11690497.0000 - val_mae: 2997.5420\n",
            "Epoch 64/350\n",
            "3/3 - 0s - loss: 8184657.5000 - mae: 2290.6133 - val_loss: 11843976.0000 - val_mae: 2964.5608\n",
            "Epoch 65/350\n",
            "3/3 - 0s - loss: 7685706.0000 - mae: 2168.0715 - val_loss: 11966941.0000 - val_mae: 2921.8738\n",
            "Epoch 66/350\n",
            "3/3 - 0s - loss: 7419828.5000 - mae: 2116.9744 - val_loss: 11764349.0000 - val_mae: 2943.1843\n",
            "Epoch 67/350\n",
            "3/3 - 0s - loss: 7542504.0000 - mae: 2174.2329 - val_loss: 9491075.0000 - val_mae: 2621.7454\n",
            "Epoch 68/350\n",
            "3/3 - 0s - loss: 7645106.5000 - mae: 2183.9221 - val_loss: 9260781.0000 - val_mae: 2576.6912\n",
            "Epoch 69/350\n",
            "3/3 - 0s - loss: 7083541.5000 - mae: 2160.8516 - val_loss: 8957292.0000 - val_mae: 2568.5420\n",
            "Epoch 70/350\n",
            "3/3 - 0s - loss: 7474798.0000 - mae: 2256.0037 - val_loss: 10407171.0000 - val_mae: 2596.1809\n",
            "Epoch 71/350\n",
            "3/3 - 0s - loss: 7420845.5000 - mae: 2154.8242 - val_loss: 10299352.0000 - val_mae: 2524.6794\n",
            "Epoch 72/350\n",
            "3/3 - 0s - loss: 7781120.5000 - mae: 2237.1011 - val_loss: 9930337.0000 - val_mae: 2518.6816\n",
            "Epoch 73/350\n",
            "3/3 - 0s - loss: 7614097.5000 - mae: 2131.1514 - val_loss: 10462601.0000 - val_mae: 2634.6169\n",
            "Epoch 74/350\n",
            "3/3 - 0s - loss: 7562501.5000 - mae: 2109.0981 - val_loss: 11283135.0000 - val_mae: 2678.9990\n",
            "Epoch 75/350\n",
            "3/3 - 0s - loss: 8832001.0000 - mae: 2289.3696 - val_loss: 11223627.0000 - val_mae: 2695.4075\n",
            "Epoch 76/350\n",
            "3/3 - 0s - loss: 7946360.0000 - mae: 2229.4319 - val_loss: 11916056.0000 - val_mae: 2804.6025\n",
            "Epoch 77/350\n",
            "3/3 - 0s - loss: 8161394.0000 - mae: 2209.4956 - val_loss: 11228249.0000 - val_mae: 2647.0325\n",
            "Epoch 78/350\n",
            "3/3 - 0s - loss: 8713288.0000 - mae: 2354.5894 - val_loss: 11292267.0000 - val_mae: 2606.8191\n",
            "Epoch 79/350\n",
            "3/3 - 0s - loss: 8981107.0000 - mae: 2373.3064 - val_loss: 10306947.0000 - val_mae: 2689.7493\n",
            "Epoch 80/350\n",
            "3/3 - 0s - loss: 8420906.0000 - mae: 2307.8340 - val_loss: 10211887.0000 - val_mae: 2673.3662\n",
            "Epoch 81/350\n",
            "3/3 - 0s - loss: 7999698.0000 - mae: 2217.6289 - val_loss: 9827761.0000 - val_mae: 2574.3252\n",
            "Epoch 82/350\n",
            "3/3 - 0s - loss: 7855786.5000 - mae: 2281.0813 - val_loss: 9739593.0000 - val_mae: 2605.6326\n",
            "Epoch 83/350\n",
            "3/3 - 0s - loss: 8449575.0000 - mae: 2212.4446 - val_loss: 9708232.0000 - val_mae: 2573.4651\n",
            "Epoch 84/350\n",
            "3/3 - 0s - loss: 8166438.5000 - mae: 2307.3972 - val_loss: 9592615.0000 - val_mae: 2545.8987\n",
            "Epoch 85/350\n",
            "3/3 - 0s - loss: 7847751.5000 - mae: 2212.5645 - val_loss: 11521661.0000 - val_mae: 2898.0574\n",
            "Epoch 86/350\n",
            "3/3 - 0s - loss: 8797221.0000 - mae: 2389.4890 - val_loss: 9947425.0000 - val_mae: 2542.8337\n",
            "Epoch 87/350\n",
            "3/3 - 0s - loss: 7488365.5000 - mae: 2118.2888 - val_loss: 11461205.0000 - val_mae: 2869.0496\n",
            "Epoch 88/350\n",
            "3/3 - 0s - loss: 7747078.5000 - mae: 2129.7734 - val_loss: 9526170.0000 - val_mae: 2531.0808\n",
            "Epoch 89/350\n",
            "3/3 - 0s - loss: 7304562.0000 - mae: 2122.7068 - val_loss: 9517699.0000 - val_mae: 2568.5901\n",
            "Epoch 90/350\n",
            "3/3 - 0s - loss: 7302140.0000 - mae: 2113.8184 - val_loss: 10023149.0000 - val_mae: 2674.2092\n",
            "Epoch 91/350\n",
            "3/3 - 0s - loss: 7143418.0000 - mae: 2087.8501 - val_loss: 9411263.0000 - val_mae: 2554.9917\n",
            "Epoch 92/350\n",
            "3/3 - 0s - loss: 7185586.0000 - mae: 2093.2810 - val_loss: 9493474.0000 - val_mae: 2570.3108\n",
            "Epoch 93/350\n",
            "3/3 - 0s - loss: 7303865.5000 - mae: 2077.0779 - val_loss: 9617043.0000 - val_mae: 2588.7441\n",
            "Epoch 94/350\n",
            "3/3 - 0s - loss: 7482277.5000 - mae: 2047.8966 - val_loss: 9604379.0000 - val_mae: 2581.1292\n",
            "Epoch 95/350\n",
            "3/3 - 0s - loss: 7634806.5000 - mae: 2158.8174 - val_loss: 9709105.0000 - val_mae: 2592.1008\n",
            "Epoch 96/350\n",
            "3/3 - 0s - loss: 7168662.5000 - mae: 2059.2173 - val_loss: 10231647.0000 - val_mae: 2705.1567\n",
            "Epoch 97/350\n",
            "3/3 - 0s - loss: 7343258.0000 - mae: 2075.0034 - val_loss: 9620123.0000 - val_mae: 2573.5518\n",
            "Epoch 98/350\n",
            "3/3 - 0s - loss: 7029164.5000 - mae: 2058.7786 - val_loss: 9935500.0000 - val_mae: 2639.6633\n",
            "Epoch 99/350\n",
            "3/3 - 0s - loss: 7215138.5000 - mae: 2044.6267 - val_loss: 9817474.0000 - val_mae: 2620.3547\n",
            "Epoch 100/350\n",
            "3/3 - 0s - loss: 7148870.0000 - mae: 2059.9961 - val_loss: 8471775.0000 - val_mae: 2424.0974\n",
            "Epoch 101/350\n",
            "3/3 - 0s - loss: 8026958.0000 - mae: 2144.2097 - val_loss: 7895712.5000 - val_mae: 2240.0613\n",
            "Epoch 102/350\n",
            "3/3 - 0s - loss: 8918777.0000 - mae: 2323.4851 - val_loss: 7975804.0000 - val_mae: 2231.9739\n",
            "Epoch 103/350\n",
            "3/3 - 0s - loss: 8329434.5000 - mae: 2195.8398 - val_loss: 10263457.0000 - val_mae: 2804.3269\n",
            "Epoch 104/350\n",
            "3/3 - 0s - loss: 9401930.0000 - mae: 2361.9392 - val_loss: 8026015.5000 - val_mae: 2158.0964\n",
            "Epoch 105/350\n",
            "3/3 - 0s - loss: 7445196.5000 - mae: 2112.6506 - val_loss: 9850571.0000 - val_mae: 2636.5139\n",
            "Epoch 106/350\n",
            "3/3 - 0s - loss: 7914214.5000 - mae: 2105.1272 - val_loss: 7843558.5000 - val_mae: 2278.6658\n",
            "Epoch 107/350\n",
            "3/3 - 0s - loss: 7490666.5000 - mae: 2114.3967 - val_loss: 7993281.5000 - val_mae: 2286.9407\n",
            "Epoch 108/350\n",
            "3/3 - 0s - loss: 7681469.5000 - mae: 2119.7847 - val_loss: 8254462.0000 - val_mae: 2379.9607\n",
            "Epoch 109/350\n",
            "3/3 - 0s - loss: 7444137.5000 - mae: 2059.4980 - val_loss: 7939858.5000 - val_mae: 2317.7400\n",
            "Epoch 110/350\n",
            "3/3 - 0s - loss: 7650940.0000 - mae: 2072.7261 - val_loss: 8265513.5000 - val_mae: 2362.7490\n",
            "Epoch 111/350\n",
            "3/3 - 0s - loss: 7512324.0000 - mae: 2079.2888 - val_loss: 8361912.0000 - val_mae: 2390.9465\n",
            "Epoch 112/350\n",
            "3/3 - 0s - loss: 7327955.5000 - mae: 2125.9668 - val_loss: 8096401.5000 - val_mae: 2338.5950\n",
            "Epoch 113/350\n",
            "3/3 - 0s - loss: 7317426.5000 - mae: 2030.9574 - val_loss: 8154454.5000 - val_mae: 2393.5537\n",
            "Epoch 114/350\n",
            "3/3 - 0s - loss: 6950928.5000 - mae: 2024.6630 - val_loss: 7793372.5000 - val_mae: 2287.6848\n",
            "Epoch 115/350\n",
            "3/3 - 0s - loss: 7111182.5000 - mae: 2063.7449 - val_loss: 7821261.5000 - val_mae: 2321.9426\n",
            "Epoch 116/350\n",
            "3/3 - 0s - loss: 6956349.5000 - mae: 2031.1389 - val_loss: 7979446.5000 - val_mae: 2332.9170\n",
            "Epoch 117/350\n",
            "3/3 - 0s - loss: 6997372.0000 - mae: 2024.8428 - val_loss: 7888473.5000 - val_mae: 2324.8320\n",
            "Epoch 118/350\n",
            "3/3 - 0s - loss: 7034765.5000 - mae: 2059.2942 - val_loss: 7833276.5000 - val_mae: 2276.0989\n",
            "Epoch 119/350\n",
            "3/3 - 0s - loss: 7080189.5000 - mae: 2032.2876 - val_loss: 7926554.5000 - val_mae: 2348.2139\n",
            "Epoch 120/350\n",
            "3/3 - 0s - loss: 7421747.5000 - mae: 2103.7612 - val_loss: 7889273.5000 - val_mae: 2370.2126\n",
            "Epoch 121/350\n",
            "3/3 - 0s - loss: 7477602.5000 - mae: 2020.2648 - val_loss: 7886384.5000 - val_mae: 2340.6667\n",
            "Epoch 122/350\n",
            "3/3 - 0s - loss: 6995884.0000 - mae: 2010.4451 - val_loss: 8277002.5000 - val_mae: 2331.5496\n",
            "Epoch 123/350\n",
            "3/3 - 0s - loss: 7010667.5000 - mae: 2055.5547 - val_loss: 8000619.5000 - val_mae: 2366.6943\n",
            "Epoch 124/350\n",
            "3/3 - 0s - loss: 7459370.5000 - mae: 2151.8430 - val_loss: 8004106.5000 - val_mae: 2406.4221\n",
            "Epoch 125/350\n",
            "3/3 - 0s - loss: 7332024.0000 - mae: 1998.0271 - val_loss: 8388991.0000 - val_mae: 2454.3621\n",
            "Epoch 126/350\n",
            "3/3 - 0s - loss: 7049070.0000 - mae: 2020.1005 - val_loss: 8582081.0000 - val_mae: 2376.1406\n",
            "Epoch 127/350\n",
            "3/3 - 0s - loss: 7412597.5000 - mae: 2103.4873 - val_loss: 8162788.0000 - val_mae: 2354.8274\n",
            "Epoch 128/350\n",
            "3/3 - 0s - loss: 7032334.5000 - mae: 2046.6969 - val_loss: 8135032.0000 - val_mae: 2324.3110\n",
            "Epoch 129/350\n",
            "3/3 - 0s - loss: 6867472.5000 - mae: 2051.5940 - val_loss: 8103889.5000 - val_mae: 2367.8757\n",
            "Epoch 130/350\n",
            "3/3 - 0s - loss: 6868457.5000 - mae: 1997.2078 - val_loss: 8184718.0000 - val_mae: 2445.1570\n",
            "Epoch 131/350\n",
            "3/3 - 0s - loss: 6728380.5000 - mae: 2006.2192 - val_loss: 8645261.0000 - val_mae: 2445.0347\n",
            "Epoch 132/350\n",
            "3/3 - 0s - loss: 6963720.0000 - mae: 2063.4126 - val_loss: 8327122.5000 - val_mae: 2441.7878\n",
            "Epoch 133/350\n",
            "3/3 - 0s - loss: 6705670.0000 - mae: 1969.7339 - val_loss: 7978138.5000 - val_mae: 2329.3911\n",
            "Epoch 134/350\n",
            "3/3 - 0s - loss: 6862803.5000 - mae: 2083.2388 - val_loss: 7993526.0000 - val_mae: 2291.5989\n",
            "Epoch 135/350\n",
            "3/3 - 0s - loss: 6954407.5000 - mae: 1995.4222 - val_loss: 8451727.0000 - val_mae: 2414.2830\n",
            "Epoch 136/350\n",
            "3/3 - 0s - loss: 6758846.5000 - mae: 1994.4475 - val_loss: 8655176.0000 - val_mae: 2423.6340\n",
            "Epoch 137/350\n",
            "3/3 - 0s - loss: 6794586.0000 - mae: 2070.2480 - val_loss: 8270444.0000 - val_mae: 2431.8098\n",
            "Epoch 138/350\n",
            "3/3 - 0s - loss: 7156476.0000 - mae: 1971.6346 - val_loss: 8295638.0000 - val_mae: 2455.9207\n",
            "Epoch 139/350\n",
            "3/3 - 0s - loss: 7297405.5000 - mae: 2134.2629 - val_loss: 8506784.0000 - val_mae: 2422.6362\n",
            "Epoch 140/350\n",
            "3/3 - 0s - loss: 6516274.5000 - mae: 1975.2428 - val_loss: 8672710.0000 - val_mae: 2509.2673\n",
            "Epoch 141/350\n",
            "3/3 - 0s - loss: 6788310.0000 - mae: 1984.0629 - val_loss: 8238473.5000 - val_mae: 2428.8491\n",
            "Epoch 142/350\n",
            "3/3 - 0s - loss: 6885825.5000 - mae: 2031.4406 - val_loss: 8172728.0000 - val_mae: 2404.2996\n",
            "Epoch 143/350\n",
            "3/3 - 0s - loss: 6772976.5000 - mae: 1961.3326 - val_loss: 8191962.5000 - val_mae: 2420.6946\n",
            "Epoch 144/350\n",
            "3/3 - 0s - loss: 6894710.0000 - mae: 2036.7421 - val_loss: 8456539.0000 - val_mae: 2425.0601\n",
            "Epoch 145/350\n",
            "3/3 - 0s - loss: 7124019.5000 - mae: 2044.0798 - val_loss: 8612317.0000 - val_mae: 2529.4971\n",
            "Epoch 146/350\n",
            "3/3 - 0s - loss: 6759007.5000 - mae: 1974.3223 - val_loss: 8885587.0000 - val_mae: 2473.9441\n",
            "Epoch 147/350\n",
            "3/3 - 0s - loss: 7381667.5000 - mae: 2064.3296 - val_loss: 8336361.5000 - val_mae: 2450.4573\n",
            "Epoch 148/350\n",
            "3/3 - 0s - loss: 6712066.0000 - mae: 1979.4810 - val_loss: 8710088.0000 - val_mae: 2431.9514\n",
            "Epoch 149/350\n",
            "3/3 - 0s - loss: 7037244.0000 - mae: 2103.9521 - val_loss: 8205682.5000 - val_mae: 2417.3918\n",
            "Epoch 150/350\n",
            "3/3 - 0s - loss: 7403519.5000 - mae: 1996.4788 - val_loss: 8496301.0000 - val_mae: 2488.6541\n",
            "Epoch 151/350\n",
            "3/3 - 0s - loss: 8568048.0000 - mae: 2289.7427 - val_loss: 9461231.0000 - val_mae: 2480.3394\n",
            "Epoch 152/350\n",
            "3/3 - 0s - loss: 6634831.5000 - mae: 1953.5004 - val_loss: 9828709.0000 - val_mae: 2610.1538\n",
            "Epoch 153/350\n",
            "3/3 - 0s - loss: 7113838.5000 - mae: 1996.9756 - val_loss: 9343624.0000 - val_mae: 2446.0959\n",
            "Epoch 154/350\n",
            "3/3 - 0s - loss: 7458932.0000 - mae: 2216.3892 - val_loss: 8315810.0000 - val_mae: 2442.6370\n",
            "Epoch 155/350\n",
            "3/3 - 0s - loss: 7175248.5000 - mae: 2056.7441 - val_loss: 8701760.0000 - val_mae: 2484.8237\n",
            "Epoch 156/350\n",
            "3/3 - 0s - loss: 6922679.5000 - mae: 2017.3441 - val_loss: 9730735.0000 - val_mae: 2494.1858\n",
            "Epoch 157/350\n",
            "3/3 - 0s - loss: 7843981.5000 - mae: 2132.9417 - val_loss: 8561867.0000 - val_mae: 2491.4607\n",
            "Epoch 158/350\n",
            "3/3 - 0s - loss: 6707565.5000 - mae: 1981.8226 - val_loss: 9477024.0000 - val_mae: 2459.1838\n",
            "Epoch 159/350\n",
            "3/3 - 0s - loss: 7103314.5000 - mae: 2052.3845 - val_loss: 8363041.5000 - val_mae: 2367.1672\n",
            "Epoch 160/350\n",
            "3/3 - 0s - loss: 6958885.5000 - mae: 2017.4932 - val_loss: 8440856.0000 - val_mae: 2410.0972\n",
            "Epoch 161/350\n",
            "3/3 - 0s - loss: 6500430.5000 - mae: 1998.1195 - val_loss: 8354449.5000 - val_mae: 2424.0793\n",
            "Epoch 162/350\n",
            "3/3 - 0s - loss: 6600187.5000 - mae: 1979.8268 - val_loss: 8433183.0000 - val_mae: 2500.0437\n",
            "Epoch 163/350\n",
            "3/3 - 0s - loss: 7231516.0000 - mae: 1956.4911 - val_loss: 8316277.5000 - val_mae: 2410.6316\n",
            "Epoch 164/350\n",
            "3/3 - 0s - loss: 6858112.5000 - mae: 2045.1069 - val_loss: 8882739.0000 - val_mae: 2402.7468\n",
            "Epoch 165/350\n",
            "3/3 - 0s - loss: 6912244.0000 - mae: 2030.7797 - val_loss: 8295057.5000 - val_mae: 2449.5583\n",
            "Epoch 166/350\n",
            "3/3 - 0s - loss: 6567014.5000 - mae: 1919.1876 - val_loss: 8926938.0000 - val_mae: 2455.7964\n",
            "Epoch 167/350\n",
            "3/3 - 0s - loss: 7181158.0000 - mae: 2199.3977 - val_loss: 8282608.0000 - val_mae: 2408.3679\n",
            "Epoch 168/350\n",
            "3/3 - 0s - loss: 7091865.5000 - mae: 2051.0552 - val_loss: 8701791.0000 - val_mae: 2452.2898\n",
            "Epoch 169/350\n",
            "3/3 - 0s - loss: 6557320.0000 - mae: 1944.3164 - val_loss: 10477303.0000 - val_mae: 2579.5449\n",
            "Epoch 170/350\n",
            "3/3 - 0s - loss: 7469365.5000 - mae: 2165.4609 - val_loss: 8391313.0000 - val_mae: 2442.7559\n",
            "Epoch 171/350\n",
            "3/3 - 0s - loss: 6874241.5000 - mae: 1922.2648 - val_loss: 8401283.0000 - val_mae: 2455.9893\n",
            "Epoch 172/350\n",
            "3/3 - 0s - loss: 6702656.5000 - mae: 2037.8817 - val_loss: 8986691.0000 - val_mae: 2438.9768\n",
            "Epoch 173/350\n",
            "3/3 - 0s - loss: 6712169.5000 - mae: 2017.3704 - val_loss: 8568697.0000 - val_mae: 2493.8132\n",
            "Epoch 174/350\n",
            "3/3 - 0s - loss: 6722481.5000 - mae: 1924.7242 - val_loss: 8761975.0000 - val_mae: 2444.7742\n",
            "Epoch 175/350\n",
            "3/3 - 0s - loss: 7052645.5000 - mae: 2059.4114 - val_loss: 8410175.0000 - val_mae: 2421.6606\n",
            "Epoch 176/350\n",
            "3/3 - 0s - loss: 6822762.5000 - mae: 2051.7068 - val_loss: 8335269.5000 - val_mae: 2394.5442\n",
            "Epoch 177/350\n",
            "3/3 - 0s - loss: 6828656.0000 - mae: 1973.2794 - val_loss: 8222942.5000 - val_mae: 2404.2432\n",
            "Epoch 178/350\n",
            "3/3 - 0s - loss: 6528670.5000 - mae: 1970.3810 - val_loss: 8986516.0000 - val_mae: 2478.9934\n",
            "Epoch 179/350\n",
            "3/3 - 0s - loss: 6555685.5000 - mae: 2003.2311 - val_loss: 8324568.5000 - val_mae: 2446.0593\n",
            "Epoch 180/350\n",
            "3/3 - 0s - loss: 6603726.5000 - mae: 1946.8159 - val_loss: 8368595.5000 - val_mae: 2414.7749\n",
            "Epoch 181/350\n",
            "3/3 - 0s - loss: 6496236.0000 - mae: 1951.1499 - val_loss: 8347964.0000 - val_mae: 2381.4753\n",
            "Epoch 182/350\n",
            "3/3 - 0s - loss: 6872773.5000 - mae: 2020.4976 - val_loss: 8562588.0000 - val_mae: 2443.8721\n",
            "Epoch 183/350\n",
            "3/3 - 0s - loss: 6523510.5000 - mae: 1966.0100 - val_loss: 8508935.0000 - val_mae: 2439.4011\n",
            "Epoch 184/350\n",
            "3/3 - 0s - loss: 6485183.5000 - mae: 2007.1334 - val_loss: 8359721.5000 - val_mae: 2408.4856\n",
            "Epoch 185/350\n",
            "3/3 - 0s - loss: 6326578.5000 - mae: 1936.9130 - val_loss: 8272024.5000 - val_mae: 2378.2698\n",
            "Epoch 186/350\n",
            "3/3 - 0s - loss: 6375486.0000 - mae: 1925.6545 - val_loss: 8437497.0000 - val_mae: 2416.5039\n",
            "Epoch 187/350\n",
            "3/3 - 0s - loss: 6517080.5000 - mae: 1949.0616 - val_loss: 8464957.0000 - val_mae: 2416.4929\n",
            "Epoch 188/350\n",
            "3/3 - 0s - loss: 6855042.5000 - mae: 1975.1648 - val_loss: 8454454.0000 - val_mae: 2441.9644\n",
            "Epoch 189/350\n",
            "3/3 - 0s - loss: 6683405.5000 - mae: 2025.2767 - val_loss: 9060161.0000 - val_mae: 2488.7170\n",
            "Epoch 190/350\n",
            "3/3 - 0s - loss: 6568267.5000 - mae: 1965.2771 - val_loss: 8455243.0000 - val_mae: 2458.1208\n",
            "Epoch 191/350\n",
            "3/3 - 0s - loss: 6382942.0000 - mae: 1928.9080 - val_loss: 9088543.0000 - val_mae: 2425.5254\n",
            "Epoch 192/350\n",
            "3/3 - 0s - loss: 6915772.0000 - mae: 2063.2456 - val_loss: 8230018.5000 - val_mae: 2364.8931\n",
            "Epoch 193/350\n",
            "3/3 - 0s - loss: 6873799.5000 - mae: 1965.4155 - val_loss: 8364518.5000 - val_mae: 2451.8120\n",
            "Epoch 194/350\n",
            "3/3 - 0s - loss: 7496370.5000 - mae: 2128.7490 - val_loss: 9201224.0000 - val_mae: 2487.7197\n",
            "Epoch 195/350\n",
            "3/3 - 0s - loss: 8802611.0000 - mae: 2219.6846 - val_loss: 9126785.0000 - val_mae: 2542.3826\n",
            "Epoch 196/350\n",
            "3/3 - 0s - loss: 7793532.0000 - mae: 2180.5881 - val_loss: 12556015.0000 - val_mae: 2800.9109\n",
            "Epoch 197/350\n",
            "3/3 - 0s - loss: 6866318.5000 - mae: 2176.8152 - val_loss: 9930374.0000 - val_mae: 2618.1582\n",
            "Epoch 198/350\n",
            "3/3 - 0s - loss: 7636358.5000 - mae: 2099.1628 - val_loss: 9047713.0000 - val_mae: 2434.2461\n",
            "Epoch 199/350\n",
            "3/3 - 0s - loss: 7605700.5000 - mae: 2256.0605 - val_loss: 8984664.0000 - val_mae: 2433.7644\n",
            "Epoch 200/350\n",
            "3/3 - 0s - loss: 7469228.0000 - mae: 2030.0676 - val_loss: 8843248.0000 - val_mae: 2468.5325\n",
            "Epoch 201/350\n",
            "3/3 - 0s - loss: 6183370.5000 - mae: 1922.5651 - val_loss: 10911023.0000 - val_mae: 2685.4937\n",
            "Epoch 202/350\n",
            "3/3 - 0s - loss: 7290754.5000 - mae: 2212.9099 - val_loss: 8646072.0000 - val_mae: 2464.5464\n",
            "Epoch 203/350\n",
            "3/3 - 0s - loss: 6549640.0000 - mae: 1934.9764 - val_loss: 8730082.0000 - val_mae: 2486.5564\n",
            "Epoch 204/350\n",
            "3/3 - 0s - loss: 6414082.5000 - mae: 1966.0511 - val_loss: 9447051.0000 - val_mae: 2497.2861\n",
            "Epoch 205/350\n",
            "3/3 - 0s - loss: 6595069.5000 - mae: 1993.2021 - val_loss: 8649231.0000 - val_mae: 2475.0232\n",
            "Epoch 206/350\n",
            "3/3 - 0s - loss: 6601137.5000 - mae: 1950.6077 - val_loss: 9158465.0000 - val_mae: 2496.7646\n",
            "Epoch 207/350\n",
            "3/3 - 0s - loss: 7456996.5000 - mae: 2199.4470 - val_loss: 9158943.0000 - val_mae: 2495.1726\n",
            "Epoch 208/350\n",
            "3/3 - 0s - loss: 8925037.0000 - mae: 2307.7341 - val_loss: 9054912.0000 - val_mae: 2518.0642\n",
            "Epoch 209/350\n",
            "3/3 - 0s - loss: 7531075.5000 - mae: 2130.5505 - val_loss: 11699372.0000 - val_mae: 2808.4792\n",
            "Epoch 210/350\n",
            "3/3 - 0s - loss: 7822088.0000 - mae: 2179.3840 - val_loss: 10050135.0000 - val_mae: 2666.3430\n",
            "Epoch 211/350\n",
            "3/3 - 0s - loss: 7128253.5000 - mae: 2030.1089 - val_loss: 9685612.0000 - val_mae: 2508.3835\n",
            "Epoch 212/350\n",
            "3/3 - 0s - loss: 6799478.0000 - mae: 2068.1577 - val_loss: 8778416.0000 - val_mae: 2427.7449\n",
            "Epoch 213/350\n",
            "3/3 - 0s - loss: 6513357.5000 - mae: 1968.4044 - val_loss: 8706935.0000 - val_mae: 2518.9966\n",
            "Epoch 214/350\n",
            "3/3 - 0s - loss: 6531888.0000 - mae: 1910.8297 - val_loss: 8889219.0000 - val_mae: 2463.4778\n",
            "Epoch 215/350\n",
            "3/3 - 0s - loss: 6408500.0000 - mae: 1951.7631 - val_loss: 8609376.0000 - val_mae: 2436.3506\n",
            "Epoch 216/350\n",
            "3/3 - 0s - loss: 6371988.0000 - mae: 1918.8872 - val_loss: 8622716.0000 - val_mae: 2437.2620\n",
            "Epoch 217/350\n",
            "3/3 - 0s - loss: 6284113.5000 - mae: 1967.7955 - val_loss: 9088427.0000 - val_mae: 2501.0925\n",
            "Epoch 218/350\n",
            "3/3 - 0s - loss: 6108176.0000 - mae: 1924.1390 - val_loss: 8425791.0000 - val_mae: 2451.3914\n",
            "Epoch 219/350\n",
            "3/3 - 0s - loss: 6624010.5000 - mae: 1932.2395 - val_loss: 8698563.0000 - val_mae: 2436.5671\n",
            "Epoch 220/350\n",
            "3/3 - 0s - loss: 6306710.5000 - mae: 1960.4426 - val_loss: 8794389.0000 - val_mae: 2465.5422\n",
            "Epoch 221/350\n",
            "3/3 - 0s - loss: 6197586.0000 - mae: 1950.7589 - val_loss: 8459223.0000 - val_mae: 2459.4719\n",
            "Epoch 222/350\n",
            "3/3 - 0s - loss: 6378108.0000 - mae: 1921.6011 - val_loss: 8403155.0000 - val_mae: 2442.6169\n",
            "Epoch 223/350\n",
            "3/3 - 0s - loss: 6388781.5000 - mae: 1971.6119 - val_loss: 8970613.0000 - val_mae: 2468.8643\n",
            "Epoch 224/350\n",
            "3/3 - 0s - loss: 6650878.0000 - mae: 1971.0848 - val_loss: 8342068.0000 - val_mae: 2402.3264\n",
            "Epoch 225/350\n",
            "3/3 - 0s - loss: 6254786.5000 - mae: 1942.1240 - val_loss: 9085349.0000 - val_mae: 2476.9841\n",
            "Epoch 226/350\n",
            "3/3 - 0s - loss: 6156443.5000 - mae: 1956.2169 - val_loss: 8607352.0000 - val_mae: 2456.1536\n",
            "Epoch 227/350\n",
            "3/3 - 0s - loss: 6537826.0000 - mae: 1908.8032 - val_loss: 8974343.0000 - val_mae: 2516.3408\n",
            "Epoch 228/350\n",
            "3/3 - 0s - loss: 6496280.0000 - mae: 2012.3521 - val_loss: 9169987.0000 - val_mae: 2511.9827\n",
            "Epoch 229/350\n",
            "3/3 - 0s - loss: 6270976.5000 - mae: 1958.6566 - val_loss: 8429705.0000 - val_mae: 2427.2019\n",
            "Epoch 230/350\n",
            "3/3 - 0s - loss: 6661675.5000 - mae: 1961.3510 - val_loss: 8867775.0000 - val_mae: 2445.7932\n",
            "Epoch 231/350\n",
            "3/3 - 0s - loss: 6229991.5000 - mae: 1931.8438 - val_loss: 8435388.0000 - val_mae: 2404.0762\n",
            "Epoch 232/350\n",
            "3/3 - 0s - loss: 6258688.0000 - mae: 1920.8040 - val_loss: 8824835.0000 - val_mae: 2482.5986\n",
            "Epoch 233/350\n",
            "3/3 - 0s - loss: 6251122.5000 - mae: 1925.7845 - val_loss: 8845639.0000 - val_mae: 2512.1008\n",
            "Epoch 234/350\n",
            "3/3 - 0s - loss: 6501672.0000 - mae: 1916.7021 - val_loss: 8856751.0000 - val_mae: 2514.3557\n",
            "Epoch 235/350\n",
            "3/3 - 0s - loss: 6175512.5000 - mae: 1923.6495 - val_loss: 9242527.0000 - val_mae: 2528.5505\n",
            "Epoch 236/350\n",
            "3/3 - 0s - loss: 6564476.5000 - mae: 1948.6648 - val_loss: 8494977.0000 - val_mae: 2398.3655\n",
            "Epoch 237/350\n",
            "3/3 - 0s - loss: 6238079.5000 - mae: 1902.1707 - val_loss: 9415567.0000 - val_mae: 2531.1848\n",
            "Epoch 238/350\n",
            "3/3 - 0s - loss: 6626848.5000 - mae: 2050.2510 - val_loss: 8764401.0000 - val_mae: 2466.4954\n",
            "Epoch 239/350\n",
            "3/3 - 0s - loss: 6568263.5000 - mae: 1993.2212 - val_loss: 8644044.0000 - val_mae: 2475.6121\n",
            "Epoch 240/350\n",
            "3/3 - 0s - loss: 6351839.5000 - mae: 1914.4087 - val_loss: 9311749.0000 - val_mae: 2522.7209\n",
            "Epoch 241/350\n",
            "3/3 - 0s - loss: 6320882.5000 - mae: 1966.7657 - val_loss: 8592868.0000 - val_mae: 2452.9272\n",
            "Epoch 242/350\n",
            "3/3 - 0s - loss: 6685942.5000 - mae: 1909.8112 - val_loss: 8539760.0000 - val_mae: 2418.9397\n",
            "Epoch 243/350\n",
            "3/3 - 0s - loss: 6326950.0000 - mae: 1903.3766 - val_loss: 10080845.0000 - val_mae: 2619.6184\n",
            "Epoch 244/350\n",
            "3/3 - 0s - loss: 6386784.0000 - mae: 1982.5768 - val_loss: 8702857.0000 - val_mae: 2473.9824\n",
            "Epoch 245/350\n",
            "3/3 - 0s - loss: 6464822.5000 - mae: 1921.0305 - val_loss: 8680407.0000 - val_mae: 2461.7539\n",
            "Epoch 246/350\n",
            "3/3 - 0s - loss: 6443823.5000 - mae: 1988.5951 - val_loss: 8665693.0000 - val_mae: 2469.2356\n",
            "Epoch 247/350\n",
            "3/3 - 0s - loss: 6111809.5000 - mae: 1907.8087 - val_loss: 8508923.0000 - val_mae: 2452.4189\n",
            "Epoch 248/350\n",
            "3/3 - 0s - loss: 6334087.5000 - mae: 1889.7906 - val_loss: 9259748.0000 - val_mae: 2525.8748\n",
            "Epoch 249/350\n",
            "3/3 - 0s - loss: 6412174.0000 - mae: 1973.2213 - val_loss: 9025508.0000 - val_mae: 2515.3333\n",
            "Epoch 250/350\n",
            "3/3 - 0s - loss: 6160658.5000 - mae: 1921.6520 - val_loss: 8822565.0000 - val_mae: 2487.5110\n",
            "Epoch 251/350\n",
            "3/3 - 0s - loss: 6208710.0000 - mae: 1894.9006 - val_loss: 8761283.0000 - val_mae: 2481.6692\n",
            "Epoch 252/350\n",
            "3/3 - 0s - loss: 6174034.0000 - mae: 1898.5945 - val_loss: 8873727.0000 - val_mae: 2488.6733\n",
            "Epoch 253/350\n",
            "3/3 - 0s - loss: 6307821.5000 - mae: 1925.8019 - val_loss: 8719509.0000 - val_mae: 2470.5071\n",
            "Epoch 254/350\n",
            "3/3 - 0s - loss: 6581442.5000 - mae: 1855.6494 - val_loss: 8624956.0000 - val_mae: 2465.2986\n",
            "Epoch 255/350\n",
            "3/3 - 0s - loss: 6462146.5000 - mae: 1977.1887 - val_loss: 9734400.0000 - val_mae: 2601.5781\n",
            "Epoch 256/350\n",
            "3/3 - 0s - loss: 6261089.5000 - mae: 1932.0922 - val_loss: 8582697.0000 - val_mae: 2434.0730\n",
            "Epoch 257/350\n",
            "3/3 - 0s - loss: 6457611.5000 - mae: 1946.1405 - val_loss: 9360673.0000 - val_mae: 2560.1216\n",
            "Epoch 258/350\n",
            "3/3 - 0s - loss: 6315708.0000 - mae: 1952.9655 - val_loss: 8632905.0000 - val_mae: 2475.7930\n",
            "Epoch 259/350\n",
            "3/3 - 0s - loss: 6275066.0000 - mae: 1885.9935 - val_loss: 8816389.0000 - val_mae: 2514.1995\n",
            "Epoch 260/350\n",
            "3/3 - 0s - loss: 6453489.5000 - mae: 1974.8314 - val_loss: 9055455.0000 - val_mae: 2510.1155\n",
            "Epoch 261/350\n",
            "3/3 - 0s - loss: 6418693.5000 - mae: 1885.3312 - val_loss: 8518872.0000 - val_mae: 2387.3628\n",
            "Epoch 262/350\n",
            "3/3 - 0s - loss: 6342504.0000 - mae: 1926.3966 - val_loss: 9589414.0000 - val_mae: 2581.9871\n",
            "Epoch 263/350\n",
            "3/3 - 0s - loss: 6592013.5000 - mae: 1960.3898 - val_loss: 8700501.0000 - val_mae: 2483.6492\n",
            "Epoch 264/350\n",
            "3/3 - 0s - loss: 6118876.0000 - mae: 1870.6743 - val_loss: 9318467.0000 - val_mae: 2573.7900\n",
            "Epoch 265/350\n",
            "3/3 - 0s - loss: 6362068.0000 - mae: 1935.8867 - val_loss: 8685608.0000 - val_mae: 2471.4565\n",
            "Epoch 266/350\n",
            "3/3 - 0s - loss: 6061049.5000 - mae: 1869.9244 - val_loss: 8830574.0000 - val_mae: 2492.3672\n",
            "Epoch 267/350\n",
            "3/3 - 0s - loss: 6283575.5000 - mae: 1948.5096 - val_loss: 8850191.0000 - val_mae: 2518.6138\n",
            "Epoch 268/350\n",
            "3/3 - 0s - loss: 6421094.0000 - mae: 1898.3496 - val_loss: 8734865.0000 - val_mae: 2489.7881\n",
            "Epoch 269/350\n",
            "3/3 - 0s - loss: 5966332.0000 - mae: 1873.6411 - val_loss: 9984591.0000 - val_mae: 2633.9758\n",
            "Epoch 270/350\n",
            "3/3 - 0s - loss: 6342379.5000 - mae: 1988.7014 - val_loss: 8642917.0000 - val_mae: 2499.2717\n",
            "Epoch 271/350\n",
            "3/3 - 0s - loss: 6261374.5000 - mae: 1912.5831 - val_loss: 8532259.0000 - val_mae: 2455.6565\n",
            "Epoch 272/350\n",
            "3/3 - 0s - loss: 6175449.5000 - mae: 1946.4735 - val_loss: 9121068.0000 - val_mae: 2558.1826\n",
            "Epoch 273/350\n",
            "3/3 - 0s - loss: 6328438.5000 - mae: 1946.1326 - val_loss: 8932387.0000 - val_mae: 2547.6292\n",
            "Epoch 274/350\n",
            "3/3 - 0s - loss: 6451427.5000 - mae: 1865.0920 - val_loss: 9767463.0000 - val_mae: 2608.5935\n",
            "Epoch 275/350\n",
            "3/3 - 0s - loss: 6196226.5000 - mae: 1924.0791 - val_loss: 8657918.0000 - val_mae: 2445.8457\n",
            "Epoch 276/350\n",
            "3/3 - 0s - loss: 6211588.0000 - mae: 1887.9087 - val_loss: 8524312.0000 - val_mae: 2432.9253\n",
            "Epoch 277/350\n",
            "3/3 - 0s - loss: 6150604.0000 - mae: 1939.4214 - val_loss: 9120495.0000 - val_mae: 2565.0759\n",
            "Epoch 278/350\n",
            "3/3 - 0s - loss: 6128536.5000 - mae: 1932.4478 - val_loss: 8842028.0000 - val_mae: 2509.1870\n",
            "Epoch 279/350\n",
            "3/3 - 0s - loss: 6265400.5000 - mae: 1865.9591 - val_loss: 8976711.0000 - val_mae: 2548.9751\n",
            "Epoch 280/350\n",
            "3/3 - 0s - loss: 6228998.0000 - mae: 1932.8439 - val_loss: 9035803.0000 - val_mae: 2530.6050\n",
            "Epoch 281/350\n",
            "3/3 - 0s - loss: 5991235.5000 - mae: 1870.2917 - val_loss: 8690433.0000 - val_mae: 2422.3450\n",
            "Epoch 282/350\n",
            "3/3 - 0s - loss: 6048344.0000 - mae: 1891.1685 - val_loss: 8818908.0000 - val_mae: 2474.7734\n",
            "Epoch 283/350\n",
            "3/3 - 0s - loss: 6114254.5000 - mae: 1875.0405 - val_loss: 9094685.0000 - val_mae: 2544.7024\n",
            "Epoch 284/350\n",
            "3/3 - 0s - loss: 6227420.5000 - mae: 1938.5920 - val_loss: 8785854.0000 - val_mae: 2498.4355\n",
            "Epoch 285/350\n",
            "3/3 - 0s - loss: 6125878.5000 - mae: 1842.9674 - val_loss: 8429465.0000 - val_mae: 2427.5818\n",
            "Epoch 286/350\n",
            "3/3 - 0s - loss: 6161303.5000 - mae: 1881.1935 - val_loss: 9362425.0000 - val_mae: 2574.7847\n",
            "Epoch 287/350\n",
            "3/3 - 0s - loss: 6110236.0000 - mae: 1913.3781 - val_loss: 8593978.0000 - val_mae: 2477.2192\n",
            "Epoch 288/350\n",
            "3/3 - 0s - loss: 6039381.5000 - mae: 1850.8370 - val_loss: 8684415.0000 - val_mae: 2502.5127\n",
            "Epoch 289/350\n",
            "3/3 - 0s - loss: 6190402.5000 - mae: 1875.8126 - val_loss: 9380098.0000 - val_mae: 2602.9280\n",
            "Epoch 290/350\n",
            "3/3 - 0s - loss: 5995444.5000 - mae: 1889.7661 - val_loss: 8637365.0000 - val_mae: 2499.3220\n",
            "Epoch 291/350\n",
            "3/3 - 0s - loss: 6110525.5000 - mae: 1876.5864 - val_loss: 8527464.0000 - val_mae: 2459.0068\n",
            "Epoch 292/350\n",
            "3/3 - 0s - loss: 6068886.5000 - mae: 1856.1056 - val_loss: 9118011.0000 - val_mae: 2590.9448\n",
            "Epoch 293/350\n",
            "3/3 - 0s - loss: 6203042.0000 - mae: 1946.7229 - val_loss: 8811223.0000 - val_mae: 2523.9653\n",
            "Epoch 294/350\n",
            "3/3 - 0s - loss: 6032852.0000 - mae: 1827.7922 - val_loss: 8538749.0000 - val_mae: 2439.4387\n",
            "Epoch 295/350\n",
            "3/3 - 0s - loss: 6125201.5000 - mae: 1877.8375 - val_loss: 9381651.0000 - val_mae: 2592.9556\n",
            "Epoch 296/350\n",
            "3/3 - 0s - loss: 6329990.0000 - mae: 1967.5350 - val_loss: 8688492.0000 - val_mae: 2516.2771\n",
            "Epoch 297/350\n",
            "3/3 - 0s - loss: 6149715.5000 - mae: 1890.6385 - val_loss: 8377422.5000 - val_mae: 2434.4824\n",
            "Epoch 298/350\n",
            "3/3 - 0s - loss: 6075401.5000 - mae: 1897.1407 - val_loss: 9905746.0000 - val_mae: 2657.8743\n",
            "Epoch 299/350\n",
            "3/3 - 0s - loss: 6260584.0000 - mae: 1947.2620 - val_loss: 8454040.0000 - val_mae: 2445.2625\n",
            "Epoch 300/350\n",
            "3/3 - 0s - loss: 6522075.5000 - mae: 1932.6038 - val_loss: 8648589.0000 - val_mae: 2504.5073\n",
            "Epoch 301/350\n",
            "3/3 - 0s - loss: 6131325.5000 - mae: 1917.8370 - val_loss: 9119843.0000 - val_mae: 2589.1392\n",
            "Epoch 302/350\n",
            "3/3 - 0s - loss: 6017020.0000 - mae: 1879.6199 - val_loss: 8293870.5000 - val_mae: 2418.0959\n",
            "Epoch 303/350\n",
            "3/3 - 0s - loss: 6209509.5000 - mae: 1895.6017 - val_loss: 9354742.0000 - val_mae: 2599.0339\n",
            "Epoch 304/350\n",
            "3/3 - 0s - loss: 6446743.5000 - mae: 1950.4279 - val_loss: 8735049.0000 - val_mae: 2537.6394\n",
            "Epoch 305/350\n",
            "3/3 - 0s - loss: 5854608.5000 - mae: 1848.2921 - val_loss: 9274831.0000 - val_mae: 2622.4866\n",
            "Epoch 306/350\n",
            "3/3 - 0s - loss: 6037638.5000 - mae: 1902.5366 - val_loss: 8610437.0000 - val_mae: 2513.6301\n",
            "Epoch 307/350\n",
            "3/3 - 0s - loss: 5938688.0000 - mae: 1838.4211 - val_loss: 8858261.0000 - val_mae: 2540.2625\n",
            "Epoch 308/350\n",
            "3/3 - 0s - loss: 6072136.5000 - mae: 1898.1626 - val_loss: 8835752.0000 - val_mae: 2539.9041\n",
            "Epoch 309/350\n",
            "3/3 - 0s - loss: 6208642.0000 - mae: 1884.7811 - val_loss: 8564838.0000 - val_mae: 2478.6860\n",
            "Epoch 310/350\n",
            "3/3 - 0s - loss: 6373713.5000 - mae: 1913.4674 - val_loss: 9611241.0000 - val_mae: 2660.9375\n",
            "Epoch 311/350\n",
            "3/3 - 0s - loss: 6633006.5000 - mae: 1936.2125 - val_loss: 8421671.0000 - val_mae: 2428.0586\n",
            "Epoch 312/350\n",
            "3/3 - 0s - loss: 7266364.5000 - mae: 2126.9001 - val_loss: 9264474.0000 - val_mae: 2619.0261\n",
            "Epoch 313/350\n",
            "3/3 - 0s - loss: 7280272.0000 - mae: 2019.9429 - val_loss: 8655797.0000 - val_mae: 2405.7925\n",
            "Epoch 314/350\n",
            "3/3 - 0s - loss: 6431106.5000 - mae: 1973.1116 - val_loss: 12094191.0000 - val_mae: 2869.2419\n",
            "Epoch 315/350\n",
            "3/3 - 0s - loss: 6769051.5000 - mae: 2108.7224 - val_loss: 9522635.0000 - val_mae: 2543.4133\n",
            "Epoch 316/350\n",
            "3/3 - 0s - loss: 6759469.5000 - mae: 1891.4886 - val_loss: 8998431.0000 - val_mae: 2609.7983\n",
            "Epoch 317/350\n",
            "3/3 - 0s - loss: 6097763.5000 - mae: 1893.6709 - val_loss: 9515715.0000 - val_mae: 2620.8210\n",
            "Epoch 318/350\n",
            "3/3 - 0s - loss: 6202135.5000 - mae: 1949.5670 - val_loss: 8590953.0000 - val_mae: 2460.7532\n",
            "Epoch 319/350\n",
            "3/3 - 0s - loss: 6279594.0000 - mae: 1906.3462 - val_loss: 9156360.0000 - val_mae: 2588.8799\n",
            "Epoch 320/350\n",
            "3/3 - 0s - loss: 7283976.0000 - mae: 2160.7483 - val_loss: 9190610.0000 - val_mae: 2602.4729\n",
            "Epoch 321/350\n",
            "3/3 - 0s - loss: 7002042.5000 - mae: 1913.1295 - val_loss: 9226904.0000 - val_mae: 2485.6858\n",
            "Epoch 322/350\n",
            "3/3 - 0s - loss: 7089312.0000 - mae: 2072.6758 - val_loss: 12827429.0000 - val_mae: 2944.8386\n",
            "Epoch 323/350\n",
            "3/3 - 0s - loss: 6862798.5000 - mae: 2049.8013 - val_loss: 8665971.0000 - val_mae: 2385.6873\n",
            "Epoch 324/350\n",
            "3/3 - 0s - loss: 6587206.5000 - mae: 1982.7773 - val_loss: 8979231.0000 - val_mae: 2567.7097\n",
            "Epoch 325/350\n",
            "3/3 - 0s - loss: 6051012.5000 - mae: 1888.5083 - val_loss: 8808184.0000 - val_mae: 2568.3545\n",
            "Epoch 326/350\n",
            "3/3 - 0s - loss: 5888315.5000 - mae: 1878.2871 - val_loss: 8555313.0000 - val_mae: 2538.0293\n",
            "Epoch 327/350\n",
            "3/3 - 0s - loss: 6103638.5000 - mae: 1877.1755 - val_loss: 8809923.0000 - val_mae: 2553.3157\n",
            "Epoch 328/350\n",
            "3/3 - 0s - loss: 6162058.5000 - mae: 1882.0596 - val_loss: 8814892.0000 - val_mae: 2542.3999\n",
            "Epoch 329/350\n",
            "3/3 - 0s - loss: 6010673.5000 - mae: 1870.6838 - val_loss: 8569761.0000 - val_mae: 2478.0420\n",
            "Epoch 330/350\n",
            "3/3 - 0s - loss: 5893752.0000 - mae: 1852.3333 - val_loss: 8808779.0000 - val_mae: 2575.6582\n",
            "Epoch 331/350\n",
            "3/3 - 0s - loss: 5909125.5000 - mae: 1866.4535 - val_loss: 8477963.0000 - val_mae: 2532.4861\n",
            "Epoch 332/350\n",
            "3/3 - 0s - loss: 6227437.5000 - mae: 1892.6097 - val_loss: 8546661.0000 - val_mae: 2536.8560\n",
            "Epoch 333/350\n",
            "3/3 - 0s - loss: 6134818.5000 - mae: 1872.0673 - val_loss: 9423893.0000 - val_mae: 2638.3118\n",
            "Epoch 334/350\n",
            "3/3 - 0s - loss: 6371402.5000 - mae: 1922.1984 - val_loss: 8730187.0000 - val_mae: 2535.0591\n",
            "Epoch 335/350\n",
            "3/3 - 0s - loss: 6092694.0000 - mae: 1872.1306 - val_loss: 9121956.0000 - val_mae: 2607.6584\n",
            "Epoch 336/350\n",
            "3/3 - 0s - loss: 6382090.5000 - mae: 1933.4475 - val_loss: 8642944.0000 - val_mae: 2554.5127\n",
            "Epoch 337/350\n",
            "3/3 - 0s - loss: 6012690.5000 - mae: 1878.0516 - val_loss: 9335444.0000 - val_mae: 2653.4158\n",
            "Epoch 338/350\n",
            "3/3 - 0s - loss: 6135996.5000 - mae: 1889.7668 - val_loss: 8296782.5000 - val_mae: 2459.8259\n",
            "Epoch 339/350\n",
            "3/3 - 0s - loss: 6008190.0000 - mae: 1858.3276 - val_loss: 8819208.0000 - val_mae: 2573.0549\n",
            "Epoch 340/350\n",
            "3/3 - 0s - loss: 6261838.0000 - mae: 1901.8365 - val_loss: 9067837.0000 - val_mae: 2590.6790\n",
            "Epoch 341/350\n",
            "3/3 - 0s - loss: 6479875.5000 - mae: 1985.8661 - val_loss: 8874694.0000 - val_mae: 2543.5022\n",
            "Epoch 342/350\n",
            "3/3 - 0s - loss: 6330732.0000 - mae: 1863.0983 - val_loss: 8379356.0000 - val_mae: 2404.9087\n",
            "Epoch 343/350\n",
            "3/3 - 0s - loss: 6005922.5000 - mae: 1850.5280 - val_loss: 10706103.0000 - val_mae: 2748.7175\n",
            "Epoch 344/350\n",
            "3/3 - 0s - loss: 6178549.5000 - mae: 1974.7250 - val_loss: 9369027.0000 - val_mae: 2638.9082\n",
            "Epoch 345/350\n",
            "3/3 - 0s - loss: 6009491.5000 - mae: 1847.4197 - val_loss: 9219656.0000 - val_mae: 2612.0935\n",
            "Epoch 346/350\n",
            "3/3 - 0s - loss: 6123890.5000 - mae: 1947.9651 - val_loss: 8619876.0000 - val_mae: 2543.4968\n",
            "Epoch 347/350\n",
            "3/3 - 0s - loss: 6041869.5000 - mae: 1886.5203 - val_loss: 8876621.0000 - val_mae: 2521.6370\n",
            "Epoch 348/350\n",
            "3/3 - 0s - loss: 5966900.0000 - mae: 1902.9034 - val_loss: 9674269.0000 - val_mae: 2637.8333\n",
            "Epoch 349/350\n",
            "3/3 - 0s - loss: 5801750.0000 - mae: 1917.9337 - val_loss: 7849058.5000 - val_mae: 2338.2854\n",
            "Epoch 350/350\n",
            "3/3 - 0s - loss: 6145093.5000 - mae: 1864.3893 - val_loss: 8568904.0000 - val_mae: 2449.4927\n",
            "MSE: 8568904.000, RMSE: 2927.269, MAE: 2449.493\n",
            "Predicted: 17371.498\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}