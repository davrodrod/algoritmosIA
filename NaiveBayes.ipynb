{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NaiveBayes.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPVUnI1N+ICn7vLO9nDHTSh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davrodrod/algoritmosIA/blob/master/NaiveBayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDj54nfhGqaJ",
        "colab_type": "text"
      },
      "source": [
        "# Clasificador Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcehasUlHO1x",
        "colab_type": "text"
      },
      "source": [
        "Fuente: https://scikit-learn.org/stable/modules/naive_bayes.html\n",
        "\n",
        "Existen varios tipos, el primero es el Gausiano que asume que las variables X siguen distribución gausiana (su media y su desviación típica se calcula automáticamente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfXKAu5ZGqm-",
        "colab_type": "code",
        "outputId": "46184f85-2c62-4700-bf94-760ddd96ed4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\n",
        "gnb = GaussianNB()\n",
        "y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
        "print(\"Number of mislabeled points out of a total %d points : %d\" % (X_test.shape[0], (y_test != y_pred).sum()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of mislabeled points out of a total 75 points : 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uz-XKsJvIBpz",
        "colab_type": "text"
      },
      "source": [
        "Otras variantes:\n",
        "\n",
        "- Multinomial Naive Bayes (MultinomialNB). Modela cada entrada como un polinomio en el que las vbles son las features y = theta1*feature1 + theta2*feature2 + .. + thetan*featuren\n",
        "\n",
        "- Complement Naive Bayes (ComplementNB). Adaptación el MNB\n",
        "\n",
        "- Bernoulli Naive Bayes (BernoulliNB ). Los datos tienen distribución multivariable Bernuilli\n",
        "\n",
        "- CategoricalNB Categorical distribution\n",
        "\n",
        "Si existen demasiados datos tiene un partial_fit para entrenar poco a poco.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHa5qRopL3nX",
        "colab_type": "text"
      },
      "source": [
        "# Uso para procesado de textos\n",
        "\n",
        "Ejemplo, fuente: https://community.alteryx.com/t5/Data-Science-Blog/Naive-Bayes-in-Python/ba-p/138424\n",
        "\n",
        "For our example we're going to be attempting to classify whether a wikipedia page is referring to a dinosaur or a cryptid (an animal from cryptozoology. Think Lochness Monster or Bigfoot).\n",
        "\n",
        "We'll be using the text from each wikipedia article as features. What we'd expect is that certain words like \"sighting\" or \"hoax\" would be more commonly found in articles about cryptozoology, while words like \"fossil\" would be more commonly found in articles about dinosaurs.\n",
        "\n",
        " \n",
        "\n",
        "We'll do some basic word-tokenization to count the occurrences of each word and then calculate conditional probabilities for each word as it pertains to our 2 categories.\n",
        "\n",
        "\n",
        "Tokenizing and counting\n",
        " \n",
        "\n",
        "First things first. We need to turn our files full of text into something a little more mathy. The simplest way to do this is to take the bag of words approach. That just means we'll be counting how many times each word appears in each document. We'll also perform a little text normalization by removing punctuation and lowercasing the text (this means \"Hello,\" and \"hello\" will now be considered the same word).\n",
        "\n",
        " \n",
        "\n",
        "Once we've cleaned the text, we need a way to delineate words. A simple approach is to just use a good 'ole regex that splits on whitespace and punctuation: \\W+."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-ALEozdNcoF",
        "colab_type": "code",
        "outputId": "d8dc7010-93a9-49a6-9e7b-35e3ad3ab856",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "import re\n",
        "import string\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "def remove_punctuation(s):\n",
        "    \"see http://stackoverflow.com/questions/265960/best-way-to-strip-punctuation-from-a-string-in-python\"\n",
        "    table = str.maketrans(\"\",\"\", string.punctuation)\n",
        "    return s.translate(table)\n",
        "\n",
        "def tokenize(text):\n",
        "    text = remove_punctuation(text)\n",
        "    text = text.lower()\n",
        "    return re.split(\"\\W+\", text)\n",
        "\n",
        "def count_words(words):\n",
        "    wc = {}\n",
        "    for word in words:\n",
        "        wc[word] = wc.get(word, 0.0) + 1.0\n",
        "    return wc\n",
        "\n",
        "s = \"Hello my name, is Greg. My favorite food is pizza.\"\n",
        "count_words(tokenize(s))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'favorite': 1.0,\n",
              " 'food': 1.0,\n",
              " 'greg': 1.0,\n",
              " 'hello': 1.0,\n",
              " 'is': 2.0,\n",
              " 'my': 2.0,\n",
              " 'name': 1.0,\n",
              " 'pizza': 1.0}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3trjk2Dk8HSD",
        "colab_type": "text"
      },
      "source": [
        "Calculating our probabilities\n",
        " \n",
        "\n",
        "So now that we can count words, let's get cooking. The code below is going to do the following:\n",
        "\n",
        " \n",
        "\n",
        "open each document\n",
        "label it as either \"crypto\" or \"dino\" and keep track of how many of each label there are (priors)\n",
        "count the words for the document\n",
        "add those counts to the vocab, or a corpus level word count\n",
        "add those counts to the word_counts, for a category level word count"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2vJNLvB8IpM",
        "colab_type": "code",
        "outputId": "a6cfe0e3-c59c-48b1-be06-211c5599bb4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip install sh\n",
        "from sh import find\n",
        "\n",
        "# Esto es el entrenamiento\n",
        "\n",
        "# setup some structures to store our data\n",
        "vocab = {}\n",
        "word_counts = {\n",
        "    \"crypto\": {},\n",
        "    \"dino\": {}\n",
        "}\n",
        "priors = {\n",
        "    \"crypto\": 0.,\n",
        "    \"dino\": 0.\n",
        "}\n",
        "docs = []\n",
        "for f in find(\"sample_data\"):\n",
        "    f = f.strip()\n",
        "    if f.endswith(\".txt\")==False:\n",
        "        # skip non .txt files\n",
        "        continue\n",
        "    elif \"cryptid\" in f:\n",
        "        category = \"crypto\"\n",
        "    else:\n",
        "        category = \"dino\"\n",
        "    docs.append((category, f))\n",
        "    # ok time to start counting stuff...\n",
        "    priors[category] += 1\n",
        "    text = open(f).read()\n",
        "    words = tokenize(text)\n",
        "    counts = count_words(words)\n",
        "    for word, count in counts.items():\n",
        "        # if we haven't seen a word yet, let's add it to our dictionaries with a count of 0\n",
        "        if word not in vocab:\n",
        "            vocab[word] = 0.0 # use 0.0 here so Python does \"correct\" math\n",
        "        if word not in word_counts[category]:\n",
        "            word_counts[category][word] = 0.0\n",
        "        vocab[word] += count\n",
        "        word_counts[category][word] += count"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sh in /usr/local/lib/python3.6/dist-packages (1.12.14)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoBDfiJ4_Ek3",
        "colab_type": "code",
        "outputId": "703cfdcd-8c82-4347-c1ba-6eaffc786637",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "new_doc = open(\"sample_data/Yeti.txt\").read()\n",
        "words = tokenize(new_doc)\n",
        "counts = count_words(words)\n",
        "print(counts)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'the': 12.0, 'yeti': 4.0, 'ˈjɛti3': 1.0, 'or': 1.0, 'abominable': 1.0, 'snowman': 1.0, 'nepali': 1.0, 'ह': 1.0, 'मम': 1.0, 'नव': 1.0, 'lit': 1.0, 'mountain': 1.0, 'man': 1.0, 'is': 2.0, 'an': 3.0, 'apelike': 1.0, 'cryptid': 1.0, 'taller': 1.0, 'than': 2.0, 'average': 1.0, 'human': 1.0, 'that': 2.0, 'said': 1.0, 'to': 5.0, 'inhabit': 1.0, 'himalayan': 1.0, 'region': 2.0, 'of': 8.0, 'nepal': 1.0, 'and': 4.0, 'tibet4': 1.0, 'names': 1.0, 'mehteh': 1.0, 'are': 2.0, 'commonly': 1.0, 'used': 1.0, 'by': 1.0, 'people': 1.0, 'indigenous': 1.0, 'part': 1.0, 'their': 1.0, 'history': 1.0, 'mythology': 1.0, 'stories': 1.0, 'first': 1.0, 'emerged': 1.0, 'as': 2.0, 'a': 4.0, 'facet': 1.0, 'western': 1.0, 'popular': 1.0, 'culture': 1.0, 'in': 2.0, '19th': 1.0, 'century': 1.0, 'scientific': 1.0, 'community': 1.0, 'generally': 1.0, 'regards': 1.0, 'legend': 1.0, 'given': 1.0, 'lack': 1.0, 'conclusive': 1.0, 'evidence5': 1.0, 'but': 1.0, 'it': 1.0, 'remains': 1.0, 'one': 1.0, 'most': 1.0, 'famous': 1.0, 'creatures': 1.0, 'cryptozoology': 1.0, '2014': 1.0, 'however': 1.0, 'two': 1.0, 'hair': 1.0, 'samples': 1.0, 'taken': 1.0, 'from': 1.0, 'remote': 1.0, 'regions': 1.0, 'himalayas': 1.0, 'have': 1.0, 'been': 1.0, 'found': 1.0, 'show': 1.0, '100': 1.0, 'per': 1.0, 'cent': 1.0, 'genetic': 1.0, 'match': 1.0, 'prehistoric': 1.0, 'polarbearlike': 1.0, 'creature': 1.0, 'existed': 1.0, 'more': 1.0, '40000': 1.0, 'years': 1.0, 'ago': 1.0, 'oxford': 1.0, 'scientist': 1.0, 'prepares': 1.0, 'expedition': 1.0, 'find': 1.0, 'it6': 1.0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKxZpcwH99tq",
        "colab_type": "text"
      },
      "source": [
        "Alright, we've got our counts. Now we'll calculate P(word|category) for each word and multiply each of these conditional probabilities together to calculate the P(category|set of words). To prevent computational errors, we're going to perform the operations in logspace. All this means is we're going to use the log(probability) so we require fewer decimal places. More on the mystical properties of logs here and here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wh0TeDVxArw0",
        "colab_type": "code",
        "outputId": "6086df99-e046-4849-f779-ea9a26731611",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import math\n",
        "\n",
        "prior_dino = (priors[\"dino\"] / sum(priors.values()))\n",
        "prior_crypto = (priors[\"crypto\"] / sum(priors.values()))\n",
        "\n",
        "log_prob_crypto = 0.0\n",
        "log_prob_dino = 0.0\n",
        "for w, cnt in counts.items():\n",
        "    # skip words that we haven't seen before, or words less than 3 letters long\n",
        "    if not w in vocab or len(w) <= 3:\n",
        "        continue\n",
        "    # calculate the probability that the word occurs at all\n",
        "    p_word = vocab[w] / sum(vocab.values())\n",
        "    # for both categories, calculate P(word|category), or the probability a \n",
        "    # word will appear, given that we know that the document is <category>\n",
        "    p_w_given_dino = word_counts[\"dino\"].get(w, 0.0) / sum(word_counts[\"dino\"].values())\n",
        "    p_w_given_crypto = word_counts[\"crypto\"].get(w, 0.0) / sum(word_counts[\"crypto\"].values())\n",
        "    # add new probability to our running total: log_prob_<category>. if the probability \n",
        "    # is 0 (i.e. the word never appears for the category), then skip it\n",
        "    if p_w_given_dino > 0:\n",
        "        log_prob_dino += math.log(cnt * p_w_given_dino / p_word)\n",
        "    if p_w_given_crypto > 0:\n",
        "        log_prob_crypto += math.log(cnt * p_w_given_crypto / p_word)\n",
        "\n",
        "# print out the reuslts; we need to go from logspace back to \"regular\" space,\n",
        "# so we take the EXP of the log_prob (don't believe me? try this: math.exp(log(10) + log(3)))\n",
        "print(\"Score(dino)  :\", math.exp(log_prob_dino + math.log(prior_dino)))\n",
        "print(\"Score(crypto):\", math.exp(log_prob_crypto + math.log(prior_crypto)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score(dino)  : 5498324743.005088\n",
            "Score(crypto): 5.457176039492645\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}