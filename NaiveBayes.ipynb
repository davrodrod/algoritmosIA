{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NaiveBayes.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMiCLGK8fNGfBLgbsMcsUfl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davrodrod/algoritmosIA/blob/master/NaiveBayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDj54nfhGqaJ",
        "colab_type": "text"
      },
      "source": [
        "Clasificador Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcehasUlHO1x",
        "colab_type": "text"
      },
      "source": [
        "Fuente: https://scikit-learn.org/stable/modules/naive_bayes.html\n",
        "\n",
        "Existen varios tipos, el primero es el Gausiano que asume que las variables X siguen distribución gausiana (su media y su desviación típica se calcula automáticamente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfXKAu5ZGqm-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "46184f85-2c62-4700-bf94-760ddd96ed4c"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\n",
        "gnb = GaussianNB()\n",
        "y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
        "print(\"Number of mislabeled points out of a total %d points : %d\" % (X_test.shape[0], (y_test != y_pred).sum()))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of mislabeled points out of a total 75 points : 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uz-XKsJvIBpz",
        "colab_type": "text"
      },
      "source": [
        "Otras variantes:\n",
        "\n",
        "- Multinomial Naive Bayes (MultinomialNB). Modela cada entrada como un polinomio en el que las vbles son las features y = theta1*feature1 + theta2*feature2 + .. + thetan*featuren\n",
        "\n",
        "- Complement Naive Bayes (ComplementNB). Adaptación el MNB\n",
        "\n",
        "- Bernoulli Naive Bayes (BernoulliNB ). Los datos tienen distribución multivariable Bernuilli\n",
        "\n",
        "- CategoricalNB Categorical distribution\n",
        "\n",
        "Si existen demasiados datos tiene un partial_fit para entrenar poco a poco.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHa5qRopL3nX",
        "colab_type": "text"
      },
      "source": [
        "Ejemplo, fuente: https://community.alteryx.com/t5/Data-Science-Blog/Naive-Bayes-in-Python/ba-p/138424\n",
        "\n",
        "For our example we're going to be attempting to classify whether a wikipedia page is referring to a dinosaur or a cryptid (an animal from cryptozoology. Think Lochness Monster or Bigfoot).\n",
        "\n",
        "We'll be using the text from each wikipedia article as features. What we'd expect is that certain words like \"sighting\" or \"hoax\" would be more commonly found in articles about cryptozoology, while words like \"fossil\" would be more commonly found in articles about dinosaurs.\n",
        "\n",
        " \n",
        "\n",
        "We'll do some basic word-tokenization to count the occurrences of each word and then calculate conditional probabilities for each word as it pertains to our 2 categories.\n",
        "\n",
        "\n",
        "Tokenizing and counting\n",
        " \n",
        "\n",
        "First things first. We need to turn our files full of text into something a little more mathy. The simplest way to do this is to take the bag of words approach. That just means we'll be counting how many times each word appears in each document. We'll also perform a little text normalization by removing punctuation and lowercasing the text (this means \"Hello,\" and \"hello\" will now be considered the same word).\n",
        "\n",
        " \n",
        "\n",
        "Once we've cleaned the text, we need a way to delineate words. A simple approach is to just use a good 'ole regex that splits on whitespace and punctuation: \\W+."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-ALEozdNcoF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "daae8aea-22e2-4b78-ffe5-9888ffd5b87b"
      },
      "source": [
        "import re\n",
        "import string\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "def remove_punctuation(s):\n",
        "    \"see http://stackoverflow.com/questions/265960/best-way-to-strip-punctuation-from-a-string-in-python\"\n",
        "    table = str.maketrans(\"\",\"\", string.punctuation)\n",
        "    return s.translate(table)\n",
        "\n",
        "def tokenize(text):\n",
        "    text = remove_punctuation(text)\n",
        "    text = text.lower()\n",
        "    return re.split(\"\\W+\", text)\n",
        "\n",
        "def count_words(words):\n",
        "    wc = {}\n",
        "    for word in words:\n",
        "        wc[word] = wc.get(word, 0.0) + 1.0\n",
        "    return wc\n",
        "\n",
        "s = \"Hello my name, is Greg. My favorite food is pizza.\"\n",
        "count_words(tokenize(s))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'favorite': 1.0,\n",
              " 'food': 1.0,\n",
              " 'greg': 1.0,\n",
              " 'hello': 1.0,\n",
              " 'is': 2.0,\n",
              " 'my': 2.0,\n",
              " 'name': 1.0,\n",
              " 'pizza': 1.0}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    }
  ]
}